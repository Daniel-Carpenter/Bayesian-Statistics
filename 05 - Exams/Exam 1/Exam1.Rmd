---
title:    "Exam 1"
subtitle: "Bayesian Statistics"
author:   "Daniel Carpenter"
date:     "April 2022"
fontsize: 12pt
geometry: margin=1in
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
    toc_depth: 2
  # github_document:
  #   toc: yes
  #   # number_sections: yes
  #   toc_depth: 2
---
# 
# See Answers to Questions Below: 

---

<br> 

## Basic Distributional Properties

> Suppose that X∼Pois(λ=4) where λ is the average number of particles leaving an unstable nucleus per second.

### Find P(X≥8) using an R function
### Find P(X=2)
### Make a plot of the distribution of X for X=0 up to X=30
The following is for the case where Y∼N(μ=6,σ=10)

### Find P(Y≤3) using an R function
### Find P(5≤Y<8) using R
### Make an R function mynorm(a,b, mu) 
> Plot the area between a and b where b>a and below the density curve when Y is distributed as follows, Y∼N(μ,σ=10) and display a 4 dec estimate of the area. Make sure your name appears on the plot.

#### Call your function with the following parameters `mynorm(3,25,5)`, `mynorm(0,20,5)`, `mynorm(-6,10, 5)`  

---

<br> 

## Bayes Rules Proofs

### Proof of Beta Posterior  

*Adapted from JK's book - page 132 Doing Bayesian Data Analysis:* <br>  

If $\theta \sim \operatorname{Beta}(\alpha, \beta)$ and $X \sim \operatorname{Bin}(n, \theta)$, prove that $\theta \mid X \sim \operatorname{Beta}(x+\alpha, n-x+\beta)$ through proof below:    

$p(\theta \mid x) \propto p(\theta) p(x \mid \theta) = \frac{1}{B(\alpha, \beta)} \theta^{\alpha-1}(1-\theta)^{\beta-1}\left(\begin{array}{l}n \\ x\end{array}\right) \theta^{x}(1-\theta)^{n-x}$  


Note Bayes' rule  
$\underbrace{p(\theta \mid x)}_{Posterior} \propto \underbrace{p(\theta)}_{Prior} \underbrace{p(x \mid \theta)}_{Lik.} = \frac{p(z, N \mid \theta) p(\theta)}{p(z, N)} = p(\theta \mid z, N)$ <br>  

Define Bernoulli and beta distributions  
$=\underbrace{\theta^{z}(1-\theta)^{(N-z)}}_{Bernoulli \ Lik.} \underbrace{\frac{\theta^{(a-1)}(1-\theta)^{(b-1)}}{B(a, b)}}_{Beta \ Prior} / p(z, N)$ <br>  

Rearrange factors  
$=\frac{1}{B(a, b) p(z, N)} \theta^{z}(1-\theta)^{(N-z)} \theta^{(a-1)}(1-\theta)^{(b-1)}$ <br>  
$=\frac{1}{p(z, N)} \times \frac{1}{B(a, b)} \theta^{z}(1-\theta)^{(N-z)} \theta^{(a-1)}(1-\theta)^{(b-1)}$ <br>


---

### Proof of Bayes Rule w/2 Discrete Events

Prove Bayes Rule for the case of two discrete events: $p(A \mid B)=\frac{p(A) p(B \mid A)}{p(B)}$, assuming: $p(A \mid B)=\frac{p(A \cap B)}{p(B)}$

Below framework taken from pg. 101 from JK's *Doing Bayesian Data Analysis*:  

1. From the definition of conditional probability (*JK pg. 92*):  
$p(A \mid B)=\frac{p(B, A)}{p(B)}$

2. Do some algebra - Multiply both sides by $p(B)$:  
$p(A \mid B) p(B)=p(B, A)$

3. With definition: $p(B \mid A)=\frac{p(B, A)}{p(A)}$, we get:  
$p(B \mid A) p(A)=p(B, A)$

4. Since steps (2) and (3) are equal to $p(B, A)$, we can assume:  
$p(A \mid B) p(B)=p(B \mid A) p(A)$

5. Divide by $p(B)$ to get:  
$p(A \mid B)=\frac{p(B \mid A) p(A)}{p(B)}$

6. You could also show a solution where the denominator is in terms of $p(B|A)$:  
$p(A \mid B)=\frac{p(B \mid A) p(A)}{\sum_{A^{*}} p\left(B \mid A^{*}\right) p\left(A^{*}\right)}$

Note that steps 5 and 6 are equivalent to `Bayes Theorem`. Therefore, 
$P(A \mid B)=\frac{P(A, B)}{P(B)} \equiv \frac{P(A \text { and } B)}{P(B)} \equiv \frac{P(A \cap B)}{P(B)} = \frac{P(B \mid A) P(A)}{P(B)}$, or Bayes Theorem

---

<br> 

## Coin Die Simululation


---

<br> 

## SLR in Jags and OpenBUGS 