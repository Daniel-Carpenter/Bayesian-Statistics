---
title:    "Exam 1"
subtitle: "Bayesian Statistics"
author:   "Daniel Carpenter"
date:     "April 2022"
fontsize: 12pt
geometry: margin=1in
output:
  html_document:
    toc: yes
    toc_float: yes
  # github_document:
  #   toc: yes
  #   # number_sections: yes
  #   toc_depth: 2
---




## Task 2.2.1 - Proof of Bern-Beta Posterior


*Adapted from JK's book - page 132 Doing Bayesian Data Analysis:* <br>

If $\theta \sim \operatorname{Beta}(\alpha, \beta)$ and $X \sim \operatorname{Bin}(n, \theta)$, prove that $\theta \mid X \sim \operatorname{Beta}(x+\alpha, n-x+\beta)$ through proof below:  

$p(\theta \mid x) \propto p(\theta) p(x \mid \theta) = \frac{1}{B(\alpha, \beta)} \theta^{\alpha-1}(1-\theta)^{\beta-1}\left(\begin{array}{l}n \\ x\end{array}\right) \theta^{x}(1-\theta)^{n-x}$  


#### Note Bayes' rule
$\underbrace{p(\theta \mid x)}_{Posterior} \propto \underbrace{p(\theta)}_{Prior} \underbrace{p(x \mid \theta)}_{Lik.} = \frac{p(z, N \mid \theta) p(\theta)}{p(z, N)} = p(\theta \mid z, N)$ <br>  

#### Define Bernoulli and beta distributions
$=\underbrace{\theta^{z}(1-\theta)^{(N-z)}}_{Bernoulli \ Lik.} \underbrace{\frac{\theta^{(a-1)}(1-\theta)^{(b-1)}}{B(a, b)}}_{Beta \ Prior} / p(z, N)$ <br>  

#### Rearrange factors
$=\frac{1}{B(a, b) p(z, N)} \theta^{z}(1-\theta)^{(N-z)} \theta^{(a-1)}(1-\theta)^{(b-1)}$ <br>  
$=\frac{1}{p(z, N)} \times \frac{1}{B(a, b)} \theta^{z}(1-\theta)^{(N-z)} \theta^{(a-1)}(1-\theta)^{(b-1)}$ <br>


---
<br>


## Task 2.2.1 - Proof of Bayes Rule w/2 Discrete Events

Prove Bayes Rule for the case of two discrete events: $p(A \mid B)=\frac{p(A) p(B \mid A)}{p(B)}$, assuming: $p(A \mid B)=\frac{p(A \cap B)}{p(B)}$

Below framework taken from pg. 101 from JK's *Doing Bayesian Data Analysis*:  

1. From the definition of conditional probability (*JK pg. 92*):  
$p(A \mid B)=\frac{p(B, A)}{p(B)}$

2. Do some algebra - Multiply both sides by $p(B)$:  
$p(A \mid B) p(B)=p(B, A)$

3. With definition: $p(B \mid A)=\frac{p(B, A)}{p(A)}$, we get:  
$p(B \mid A) p(A)=p(B, A)$

4. Since steps (2) and (3) are equal to $p(B, A)$, we can assume:  
$p(A \mid B) p(B)=p(B \mid A) p(A)$

5. Divide by $p(B)$ to get:  
$p(A \mid B)=\frac{p(B \mid A) p(A)}{p(B)}$

6. You could also show a solution where the denominator is in terms of $p(B|A)$:  
$p(A \mid B)=\frac{p(B \mid A) p(A)}{\sum_{A^{*}} p\left(B \mid A^{*}\right) p\left(A^{*}\right)}$

Note that steps 5 and 6 are equivalent to `Bayes Theorem`. Therefore, 
$P(A \mid B)=\frac{P(A, B)}{P(B)} \equiv \frac{P(A \text { and } B)}{P(B)} \equiv \frac{P(A \cap B)}{P(B)} = \frac{P(B \mid A) P(A)}{P(B)}$, or Bayes Theorem

