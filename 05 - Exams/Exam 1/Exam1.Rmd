---
title:    "Exam 1"
subtitle: "Bayesian Statistics"
author:   "Daniel Carpenter"
date:     "April 2022"
fontsize: 12pt
geometry: margin=1in
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
    toc_depth: 2
  # github_document:
  #   toc: yes
  #   # number_sections: yes
  #   toc_depth: 2
---
# 
# See Answers to Questions Below: 

---

<br> 

## Basic Distributional Properties

> Suppose that X∼Pois(λ=4) where λ is the average number of particles leaving an unstable nucleus per second.

### Find P(X≥8) using an R function
```{r 2.1.1, echo=TRUE}
x = 8
1-ppois(x-1, lambda = 4) # x - 1 since discrete
```
```{r include=FALSE}
rm(x) # get rid of x just in case of confusion later on
```


### Find P(X=2)
```{r 2.1.2, echo=TRUE}
dpois(2, lambda = 4) # dpois() since exact probability
```

### Make a plot of the distribution of X for X=0 up to X=30
```{r 2.1.3, echo=TRUE, message=FALSE, warning=FALSE}
# Function to plot the poisson distribution (discrete)
myPoisPlot <- function(x, lambdaValue) {
  theTitle = paste('Poisson Distribution: Lambda = ', lambdaValue, 'Over Discrete Values of x\nDaniel Carpenter')
  
  probx <- dpois(x, lambda = lambdaValue)
  names(probx) <- x
  
  barplot(probx, col = paste0("lightsteelblue", floor(runif(1, min=1, max=4))), 
          main = theTitle, xlab ='Discrete Values of x', ylab ='Probability given Lambda')
}

myPoisPlot(x=0:30, lambdaValue=4)
```
The following is for the case where Y∼N(μ=6,σ=10)

```{r 2.1.4_Assumptions, echo=TRUE}
mu    = 6
sigma = 10
```


### Find P(Y≤3) using an R function
```{r 2.1.5, echo=TRUE}
pnorm(3, mu, sigma)
```

### Find P(5≤Y<8) using R
```{r 2.1.6, echo=TRUE}
# Inputs for upper and lower bound when calculating the area
upperBound = 8
lowerBound = 5

# Calculate the area in between the two bounds
pnorm(upperBound, mu, sigma) - pnorm(lowerBound, mu, sigma)
```

### Make an R function `mynorm(a, b, mu) `
> Plot the area between a and b where b>a and below the density curve when Y is distributed as follows, Y∼N(μ,σ=10) and display a 4 dec estimate of the area. Make sure your name appears on the plot.

```{r 2.1.7, echo=TRUE}
mynorm <- function(lowerBound = NA, upperBound = NA, mu, sigma=10)
{
  # Round to this many decimal places on the plot
  roundTo = 4
  
  # Color of the plot
  color = paste0("lightsteelblue", floor(runif(1, min=1, max=4)))
  
  # Calculate the "xlim" lower and upper bound for the Normal PDF Curve
  curveLowerBound <- mu - 3*sigma
  curveUpperBound <- mu + 3*sigma
  
  # Initialize variables related to output and graph
  title <- ""   # Title of graph
  exactProb = 0 # The exact probability of the questions
  
  # If no provided LOWER AND UPPER Bound (NA as parameter value) then assume none
  if (!(is.na(lowerBound)) & !(is.na(upperBound))) {
    title <- paste0(", P(", lowerBound, " <= X < ",upperBound,")")
    exactProb = pnorm(upperBound, mu, sigma) - pnorm(lowerBound, mu, sigma) # calculate prob 
    
  # If no provided LOWER Bound (NA as parameter value) then assume none
  } else if (is.na(lowerBound)) {
    lowerBound = curveLowerBound
    title <- paste0(", P(X < ",upperBound,")") # Set a dynamic title
    exactProb = pnorm(upperBound, mu, sigma) # calculate prob 
    
  # If no provided UPPER Bound (NA as parameter value) then assume none
  } else if(is.na(upperBound)) {
    upperBound = curveUpperBound
    title <- paste0(", P(X >= ",lowerBound,")")
    exactProb = 1 - pnorm(lowerBound, mu, sigma) # calculate prob 
  }
  
  # Create the line that displays the bell curve (between the CURVE bounds defined above)
  curve(
    
    ## Normally Distributed
    dnorm(x,mu,sigma), 
    
    ## Normally Distributed
    xlim=c(curveLowerBound, curveUpperBound), 
    
    ## Line width
    lwd =2, 
    
    ## Title with descriptive characteristics about function parameters
    main = paste0("Probability Distribution (by Daniel Carpenter)\n",
                  "X ~ N(",mu,", ",sigma,")", title),
    
    ## X and Y labels
    ylab = 'Probability Density',
    xlab = 'Value of X',
  )

  
  # Add the AREA of between the lower and upper bound P(lowerBound<X<=upperBound)
  
    ## X-Axis curve (length does not matter)
    xcurve = seq(lowerBound,upperBound, length=1000)
    
    ## Y-Axis Curve
    ycurve = dnorm(xcurve, mu,sigma)
    
    ## Combine the X and Y curve to form the area (in green)
    polygon(c(lowerBound, xcurve, upperBound), 
            c(0, ycurve, 0), 
            col=color) 
    
    ## Legend
    legend("topleft", legend="Area (of Prob. Density)", 
           fill=color, bty = "n")
  
  # Add the probability as text
    
    ## Calculate the area (probability)
    area = exactProb
    areaRounded = format(round(area, roundTo), nsmall=roundTo)
    
    ## Place this on the above plot
    text(12,0.02,substitute(paste("Probability = ", areaRounded), 
                            list(areaRounded = areaRounded)))
}
```


#### Call your function with the following parameters `mynorm(3,25,5)`, `mynorm(0,20,5)`, `mynorm(-6,10, 5)`  
```{r 2.1.6.1_runFun}
mynorm( 3, 25, 5)
mynorm( 0, 20, 5)
mynorm(-6, 10, 5)
```


---

<br> 

## Bayes Rules Proofs

### Proof of Beta Posterior  

*Adapted from JK's book - page 132 Doing Bayesian Data Analysis:* <br>  

If $\theta \sim \operatorname{Beta}(\alpha, \beta)$ and $X \sim \operatorname{Bin}(n, \theta)$, prove that $\theta \mid X \sim \operatorname{Beta}(x+\alpha, n-x+\beta)$ through proof below:    

$p(\theta \mid x) \propto p(\theta) p(x \mid \theta) = \frac{1}{B(\alpha, \beta)} \theta^{\alpha-1}(1-\theta)^{\beta-1}\left(\begin{array}{l}n \\ x\end{array}\right) \theta^{x}(1-\theta)^{n-x}$  


Note Bayes' rule  
$\underbrace{p(\theta \mid x)}_{Posterior} \propto \underbrace{p(\theta)}_{Prior} \underbrace{p(x \mid \theta)}_{Lik.} = \frac{p(z, N \mid \theta) p(\theta)}{p(z, N)} = p(\theta \mid z, N)$ <br>  

Define Bernoulli and beta distributions  
$=\underbrace{\theta^{z}(1-\theta)^{(N-z)}}_{Bernoulli \ Lik.} \underbrace{\frac{\theta^{(a-1)}(1-\theta)^{(b-1)}}{B(a, b)}}_{Beta \ Prior} / p(z, N)$ <br>  

Rearrange factors  
$=\frac{1}{B(a, b) p(z, N)} \theta^{z}(1-\theta)^{(N-z)} \theta^{(a-1)}(1-\theta)^{(b-1)}$ <br>  
$=\frac{1}{p(z, N)} \times \frac{1}{B(a, b)} \theta^{z}(1-\theta)^{(N-z)} \theta^{(a-1)}(1-\theta)^{(b-1)}$ <br>


---

<br>


### Proof of Bayes Rule w/2 Discrete Events

Prove Bayes Rule for the case of two discrete events: $p(A \mid B)=\frac{p(A) p(B \mid A)}{p(B)}$, assuming: $p(A \mid B)=\frac{p(A \cap B)}{p(B)}$

Below framework taken from pg. 101 from JK's *Doing Bayesian Data Analysis*:  

1. From the definition of conditional probability (*JK pg. 92*):  
$p(A \mid B)=\frac{p(B, A)}{p(B)}$

2. Do some algebra - Multiply both sides by $p(B)$:  
$p(A \mid B) p(B)=p(B, A)$

3. With definition: $p(B \mid A)=\frac{p(B, A)}{p(A)}$, we get:  
$p(B \mid A) p(A)=p(B, A)$

4. Since steps (2) and (3) are equal to $p(B, A)$, we can assume:  
$p(A \mid B) p(B)=p(B \mid A) p(A)$

5. Divide by $p(B)$ to get:  
$p(A \mid B)=\frac{p(B \mid A) p(A)}{p(B)}$

6. You could also show a solution where the denominator is in terms of $p(B|A)$:  
$p(A \mid B)=\frac{p(B \mid A) p(A)}{\sum_{A^{*}} p\left(B \mid A^{*}\right) p\left(A^{*}\right)}$

Note that steps 5 and 6 are equivalent to `Bayes Theorem`. Therefore, 
$P(A \mid B)=\frac{P(A, B)}{P(B)} \equiv \frac{P(A \text { and } B)}{P(B)} \equiv \frac{P(A \cap B)}{P(B)} = \frac{P(B \mid A) P(A)}{P(B)}$, or Bayes Theorem

---

<br> 


## Coin Die Simululation

Code for Coin Die Simulation:
```{r coinDie, echo=TRUE, message=FALSE, warning=FALSE}
coindie<-function(n=100, h=c(1/4,3/4),E2=c(5,6),init=1,...) {
  if(!require(xtable)) install.packages(xtable)
  dieset<-c()
  dieset[1]<-"E1"
  
  die<-function(n=1) {
    sample(1:6,size=n,replace=TRUE)
  }
  
  coin<-function(n=1) {
    sample(1:2,size=n,replace=TRUE)
  }
  
  face<-c()
  alpha<-c() # holds acceptance probs
  alpha[1]<-1
  post<-c()# post sample
  prop<-c() # vec of proposed states 1s and 2s
  prop[1]=init # initial state
  post[1]=prop[1]
  dice<-c()
  dice[1]<-die()
  
  for(i in 2:n) { # starts at 2 because initial value given above
    prop[i]<-coin()
    alpha[i]=min(1,h[prop[i]]/h[post[i-1]])
    
    dice[i]<-die()
    
    ifelse(alpha[i]==1,dieset[i]<-"E1",dieset[i]<-"E2")
    
    # is x an element of set y
    if(alpha[i]==1 | (is.element(dice[i],E2) & alpha[i]!=1)){post[i]<-prop[i]}
    else { post[i]<-post[i-1] }
  }  
  res<-matrix(c(prop,round(alpha,2),dieset,dice,post ),nc=5,nr=n,byrow=FALSE,dimnames=list(1:n,c("proposal","alpha", "E","dice","post")))
  sim<-table(post)/n
  postexact<-h/sum(h)
  
  # Plots
  layout(matrix(1:2, nr=1,nc=2))
  plot(post,type ="l",...)
  names(h)=c("1", "2")
  barplot(h)
  
  # Return the iterations, simulation, posterior exact and the estimate, and a table of the res
  return(list(iter=res,sim=sim,postexact=postexact,post=post,xtable=xtable(res,dig=1)) )
}

set.seed(25) # Set seed

# Call the function
coindie(n=10,h=c(0.6,0.4),E2=c(3,4,5,6)) ->ans

# Shoe the Results
ans$it
```
### When the `coindie` function is called above what is the acceptance set?
* The initial acceptance set is E1, and it finishes at E1.

### There are 10 iterations above. From the table of output `ans$it` how many times does the sampler move from state 1 to 2?

```{r fromStateToState, echo=TRUE}
POST_COLUMN = 1 # Column number that the post are in
post <- ans$it[,POST_COLUMN] # Get the states from the iterations

from1to2 = 0 # track the state changes from 1 to 2
for (i in 1:(length(post) - 1)) {
  
  # If the proposal changes from 1 to 2 (not 2 to 1), then count it
  if ((post[i] == 1) & (post[i+1] == 2)) {
    from1to2 = from1to2 + 1
  } 
}

paste('Specifically, the sampler moves from *state one* to *state two*', from1to2, 'times')

countOfChanges  = 0 # track the total number of changes
for (i in 1:(length(post) - 1)) {
  
  # If any state change occurs, then count it
  if (post[i] != post[i+1]) {
    countOfChanges = countOfChanges + 1
  } 
}

paste('The total number of times the sampler changes states is', countOfChanges, 'times')

```





### Write down the formula for the acceptance probability.

Note below is from page 33 of *Teaching MCMC* by Stewart & Stewart:

$$
\alpha_{i, j}=\min \left\{1, \frac{h_{j}}{h_{i}}\right\}
$$

where $\alpha_{i, j}$ is the probability of accepting state $j$ given that the sampler is at state $i$. 


---

<br> 

## SLR in Jags and OpenBUGS 