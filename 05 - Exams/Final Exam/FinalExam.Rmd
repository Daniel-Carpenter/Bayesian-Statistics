---
title:    "Final Exam"
subtitle: "Bayesian Statistics"
author:   "Daniel Carpenter"
date:     "May 9, 2022"
fontsize: 12pt
geometry: margin=1in
output:
  html_document:
    toc: yes
    toc_float: yes
    toc_depth: 2
  # github_document:
  #   toc: yes
  #   toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo    = TRUE,
	include = TRUE,
	message = FALSE,
	warning = FALSE,
	cache   = TRUE
)
```


# Question 1 

## 1. (a) General Posterior

*Adapted from JK's book - page 132 Doing Bayesian Data Analysis:* <br>  

If $\theta \sim \operatorname{Beta}(\alpha, \beta)$ and $X \sim \operatorname{Bin}(n, \theta)$, prove that $\theta \mid X \sim \operatorname{Beta}(x+\alpha, n-x+\beta)$ through proof below:    

$p(\theta \mid x) \propto p(\theta) p(x \mid \theta) = \frac{1}{B(\alpha, \beta)} \theta^{\alpha-1}(1-\theta)^{\beta-1}\left(\begin{array}{l}n \\ x\end{array}\right) \theta^{x}(1-\theta)^{n-x}$  


Note Bayes' rule  
$\underbrace{p(\theta \mid x)}_{Posterior} \propto \underbrace{p(\theta)}_{Prior} \underbrace{p(x \mid \theta)}_{Lik.} = \frac{p(x, n \mid \theta) p(\theta)}{p(x, n)} = p(\theta \mid x, n)$ <br>  

Define Bernoulli and beta distributions  
$=\underbrace{\theta^{x}(1-\theta)^{(n-x)}}_{Bernoulli \ Lik.} \underbrace{\frac{\theta^{(\alpha-1)}(1-\theta)^{(\beta-1)}}{B(\alpha, \beta)}}_{Beta \ Prior} / p(x, n)$ <br>  

Rearrange factors  
$=\frac{1}{B(\alpha, \beta) p(x, n)} \theta^{(\alpha-1)} (1-\theta)^{(\beta-1)} \theta^{x}(1-\theta)^{(n-x)}$ <br>

$=\frac{1}{B(\alpha, \beta)} \theta^{(\alpha-1)} (1-\theta)^{(\beta-1)} \left(\begin{array}{l}\frac{1}{p(x, n)}\end{array}\right)\theta^{x}(1-\theta)^{(n-x)}$ <br>

By definition of the binomial coefficient, which we arrive at the **solution**:  
$=\frac{1}{B(\alpha, \beta)} \theta^{(\alpha-1)} (1-\theta)^{(\beta-1)} \left(\begin{array}{l}n \\ x\end{array}\right) \theta^{x}(1-\theta)^{(n-x)}$ <br>

---

<br>

## 1. (b) If $\alpha$ = $\beta$ = 1 what prior distribution does this correspond to?
* Shape 1 and Shape two with values of 1 create a `uniform` distribution.  

## 1. (c) Suppose she uses $\alpha$ = $\beta$ = 5 for the prior what is the posterior distribution?
* Mixing a beta-prior and bernoulli likelihood Creates a `Beta` posterior`
* n=10, x=4, $\alpha$ = $\beta$ = 5
```{r 1c}
# These need to be in your document directory to load
source("DBDA2E-utilities.R")  # Load definitions of graphics functions etc.
source("BernBeta.R")          # Load the definition of the BernBeta function

# Specify the prior:
t = 4              # Specify the prior MODE.
n = 10             # Specify the effective prior sample size.
a = 5              # Convert to beta shape parameter a.
b = 5              # Convert to beta shape parameter b.

Prior = c(a,b)     # Specify Prior as vector with the two shape parameters.

# Specify the data:
N = 10                        # The total number of flips.
x = 4                         # The number of heads.
Data = c(rep(0,N-x),rep(1,x)) # Convert N and z into vector of 0's and 1's.


# Plot the three graphs, and >> VIEW THE POSTERIOR <<:
posterior = BernBeta( priorBetaAB=Prior, Data=Data , plotType="Bars" , 
                      showCentTend="Mode" , showHDI=TRUE , showpD=FALSE )
```

## 1. (d) If a further experiment is made after the first using the same coin, this time with n = 20, x = 12, what prior should the researcher use in the absence of any other information than what is given in this problem?
* Since there is no information, use hyper parameters $\alpha$ = $\beta$ = 1 for this experiment.
* In question 1(f) we will mix the betas to get a more informed result

## 1. (e) Find the posterior after the second sequential experiment.
* Mixing a beta-prior and bernoulli likelihood Creates a `Beta posterior`
* n=20, x=12, $\alpha$ = $\beta$ = 1
```{r 1e}
# These need to be in your document directory to load
source("DBDA2E-utilities.R")  # Load definitions of graphics functions etc.
source("BernBeta.R")          # Load the definition of the BernBeta function

# Specify the prior:
t = 12              # Specify the prior MODE.
n = 20             # Specify the effective prior sample size.
a = 1              # Convert to beta shape parameter a.
b = 1              # Convert to beta shape parameter b.

Prior = c(a,b)     # Specify Prior as vector with the two shape parameters.

# Specify the data:
N = 20                        # The total number of flips.
x = 12                         # The number of heads.
Data = c(rep(0,N-x),rep(1,x)) # Convert N and z into vector of 0's and 1's.


# Plot the three graphs, and >> VIEW THE POSTERIOR <<:
posterior2 = BernBeta( priorBetaAB=Prior, Data=Data , plotType="Bars" , 
                       showCentTend="Mode" , showHDI=TRUE , showpD=FALSE )
```
## 1. (f) The researcher now wishes to plot the posterior (that which is formed from the two experiments). Fill in the gaps of the code so that the correct plot is
created.  

### Mixed beta Calculation:
$$
w \times dbeta(x, a1, b1) + (1-w) \times dbeta(x, a2,b2)
$$

### Mixed Beta function
```{r 1f}
Mymixbeta <- function(w=0.5, n=10, x, a1=1, a2=1, b1=1, b2=1) {
  # Specify the prior:
  t = x                          # Specify the prior MODE.
  
  # Specify the data:
  N = n                          # The total number of flips.
  z = x                          # The number of heads.
  Data = c(rep(0,N-z),rep(1,z))  # Convert N and z into vector of 0's and 1's.
  
  # Create summary values of Data:
  z = sum( Data ) # number of 1's in Data
  N = length( Data ) 
  
  Theta = seq(0.001,0.999,by=0.001)                                                 # points for plotting
  pTheta = w*dbeta(Theta,a1,b1) +(1-w)*dbeta(Theta,a2,b2)                           # prior for plotting
  pThetaGivenData = w*dbeta(Theta, a1+z, b1+N-z) + (1-w)*dbeta(Theta, a2+z, b2+N-z) # posterior for plotting
  pDataGivenTheta = Theta^z * (1-Theta)^(N-z)                                       # likelihood for plotting
  
  
  # Plot Layout
  layout( matrix( c( 1,2,3 ) ,nrow=3 ,ncol=1 ,byrow=FALSE ) ) # 3x1 panels
  par( mar=c(3,3,1,0) , mgp=c(2,0.7,0) , mai=c(0.5,0.5,0.3,0.1) ) # margins
  cexAxis = 1.33
  cexLab = 1.75
  
  # convert plotType to notation used by plot:
  plotType="h"
  dotsize = 5 # how big to make the plotted dots
  barsize = 5 # how wide to make the bar lines   
  
  # y limits for prior and posterior:
  yLim = c(0,1.1*max(c(pTheta,pThetaGivenData)))
  
  
  # Plot the Prior
  plot( Theta , pTheta , type=plotType , 
        pch="." , cex=dotsize , lwd=barsize ,
        xlim=c(0,1) , ylim=yLim , cex.axis=cexAxis ,
        xlab=bquote(theta) , ylab='Mixed Beta' , 
        cex.lab=cexLab ,
        main="Prior (beta) - Daniel Carpenter" , cex.main=1.5 , col="skyblue")
  
  # Plot the likelihood
  plot( Theta , pDataGivenTheta , type=plotType , 
        pch="." , cex=dotsize , lwd=barsize ,
        xlim=c(0,1) , ylim=c(0,1.1*max(pDataGivenTheta)) , cex.axis=cexAxis ,
        xlab=bquote(theta) , ylab=bquote( "p(D|" * theta * ")" ) , 
        cex.lab=cexLab ,
        main="Likelihood (Bernoulli)" , cex.main=1.5 , col="skyblue" )
  
  # Plot the posterior.
  plot( Theta , pThetaGivenData , type=plotType , 
        pch="." , cex=dotsize , lwd=barsize ,
        xlim=c(0,1) , ylim=yLim , cex.axis=cexAxis ,
        xlab=bquote(theta) , ylab='Mixed Beta Posterior' , 
        cex.lab=cexLab ,
        main="Posterior (beta)" , cex.main=1.5 , col="skyblue" )
}

# Call the function and print the prior (mixed beta), likelihood and posterior)
Mymixbeta(w=0.5, n=20, x=12, a1=1, a2=1, b1=5, b2=5)
```

<!-- ### 1. f, i) A = ... -->

<!-- ### 1. f, ii) B = ... -->

---

<br>

# Question 2 - Probability

## 2. a) P(X > 0.7|α = 2, β = 3)
```{r}
pbeta(q = 0.7, 
      shape1 = 2, shape2 = 3,
      lower.tail = FALSE) # >
```

## 2. b) P(X < 0.2|α = 1, β = 1)
```{r}
pbeta(q = 0.2, 
      shape1 = 1, shape2 = 1,
      lower.tail = TRUE) # <
```

## 2. c) Find the value of X with lower tail probability 0.04. X ∼ Beta(5, 4)
```{r}
qbeta(p = 0.04, 
      shape1 = 5, shape2 = 4, 
      lower.tail = TRUE) # <

```

## 2. d) Find the equal tail interval that contains 95% of the distribution, X ∼ Beta(4, 8)
```{r}
alphaCI = 0.05 / 2  # 2.5% upper/lower
a = 4 # shape 1 
b = 8 # shape 2

# Upper tail 97.5%
upperTail = qbeta(p = alphaCI, 
                  shape1=a, shape2=b, 
                  lower.tail = FALSE) # <

# Lower tail 2.5%
lowerTail = qbeta(p = alphaCI, 
                  shape1=a, shape2=b, 
                  lower.tail = TRUE) # >
# Equal tail interval: 95% confidence between 2.5% and 97.5%
paste0('There is a ', (1 - alphaCI * 2) * 100, '% Probability that the value will fall between ', 
      round(lowerTail, 3),' and ', round(upperTail, 3))


```

## 2. e) Generate a random sample of X values of size 20 with X ∼ Beta(5, 7)
```{r}
randBetaDist <- rbeta(n = 20, 
                      shape1 = 5,
                      shape2 = 7)

# Print the data
library(knitr)
kable(randBetaDist)

```


---

<br>

# Question 3 - Interpret Posteriors

## 3. a) Give the 95% HDI for θ the probability of a success.
* There is a probability of 95% that the value will fall between 0.184 and 0.433

## 3. b) What is the posterior probability that θ < 0.5
* The posterior probability that θ < 0.5 is $99.7$%

## 3. c) If H0 : θ = 0.5 should we reject it (the NULL), yes/no?
* Yes, we should reject it because 0.5 falls outside the 95% and 90% HDI

## 3. d) The posterior probability that θ lies in (0.203, 0.413) is 0.90. True or False?
* It is `TRUE` that there is a probability of 90% that the value will fall between 0.203 and 0.413


## 3. e) What does ROPE stand for?
* ROPE stands for “Region Of Practical Equivalence”. In bayesian analysis, if the ROPE is entirely within the HDI for θ then we should reject the NULL for practical purposes.

---

<br>


# Question 4 - GLM

## 4. a) Link function?
* There is a logit link function used. Model:

$$
logit(p) = log (p/(1 − p))
$$

## 4. b) What numerical values will replace the y variable?
* The value of y is transformed onto a new scale, and that transformed value is modeled as a linear combination of predictors. 
* Maps theta *from* `0-1` *to* the real value between $-\infty$ and $\infty$

## 4. c) High impact priors?
* They are `low` impact priors
* It means that the prior distribution will not impact the posterior significantly

## 4. d) Interpret Point/Interval Estimates

### 4. d, 1) $\beta_3$ Point Estimate
* The point estimate for $\beta_3$ is -0.75905 with a standard deviation of 0.199376. 
* I.e. a male is more likely to die than a female.

### 4. d, 2) $\beta_1$ Interval Estimate
* There is a 95% probability that the values of $\beta_1$ will fall between 0.025778 and 0.6118

---

<br>


# Question 5 - Exponential Family

The exponential family can be defined as any density of the following form:
$$
f(y \mid \theta, \phi)=\exp \left(\frac{y \theta-b(\theta)}{a(\phi)}+c(y, \phi)\right)
$$
<br>

## 5. a) Show $E(Y) = b^{\prime}(\theta)$

### Sum of distribution is 1
$$
\int_y f(y)dy = 1 \\
$$

### Differentiate above equation
$$
\frac{d}{d\theta} \int_y f(y)dy = \frac{d}{d\theta} 1 = 0
$$

### Take inside the integral
$$
\int_y \frac{df}{d\theta} dy = 0
$$

### Differentiate with $f$ being the exponential
$$
\frac{df}{d\theta} = 
\frac{y -b^{\prime}(\theta)}{a(\phi)} \ 
\exp \left(\frac{y \theta-b(\theta)}{a(\phi)}
+ c(y, \phi)\right)
$$

### Replace with $f$ since it is the expontial
$$
\frac{df}{d\theta} = 
\frac{y -b^{\prime}(\theta)}{a(\phi)} \ f
$$

### Plug $\int_y \frac{df}{d\theta} dy = 0$ into above $\frac{df}{d\theta}$
$$
\int_y \begin{bmatrix} \frac{y -b^{\prime}(\theta)}{a(\phi)} \end{bmatrix} \ f \ dy = 0
$$

### Distribute $f$
$$
\int_y \begin{bmatrix} \frac{yf -b^{\prime}(\theta)f}{a(\phi)} \end{bmatrix} \ f \ dy = 0
$$

### Multiply by $a(\phi)$ to get rid of it
$$
\int_y yf \ dy 
- \int_y b^{\prime}(\theta)f \ dy= 0
$$

### Writing above in terms of E(Y)
$$
E(Y) - b^{\prime}(\theta) \ \int_yf \ dy= 0
$$

### *Solution*: Therefore, the expected value of Y $E(Y)$ is $b^{\prime}(\theta)$
$$
E(Y) = b^{\prime}(\theta)
$$

---

<br>


## 5. b) Show $V(Y) = b^{\prime \prime}(\theta) a(\phi)$

### From the prior proof above
$$
\frac{df}{d\theta} = 
\begin{bmatrix} \frac{y -b^{\prime}(\theta)}{a(\phi)} \end{bmatrix} \ f
$$

### Get the second derivative of above
$$
\frac{d^2f}{d\theta^2} = 
\frac{-b^{\prime \prime}(\theta)}{a(\phi)} \ f
+ \frac{y -b^{\prime}(\theta)}{a(\phi)} \ f^{\prime}
$$

### Replace above $f^{\prime}$ with $f$ from prior proof (two above points)
$$
\frac{d^2f}{d\theta^2} = 
\frac{-b^{\prime \prime}(\theta)}{a(\phi)} \ f
+ \begin{bmatrix} \frac{y -b^{\prime}(\theta)}{a(\phi)} \end{bmatrix}^2 \ f
$$

### Note sum is 1 for all densities
$$
\int_y dy = 1 \\
$$

### Take 2nd dertivative using above notes to get 0
$$
\int_y \frac{d^2f}{d\theta^2} \ dy = 0
$$

### Plug $\frac{d^2f}{d\theta^2}$ into above equation
$$
\int_y
\begin{pmatrix}
\frac{-b^{\prime \prime}(\theta)}{a(\phi)} \ f
+ \begin{bmatrix} \frac{y -b^{\prime}(\theta)}{a(\phi)} \end{bmatrix}^2 \ f
\end{pmatrix} \
dy = 0
$$

### Simplify prior equation and isolate $\frac{-b^{\prime \prime}(\theta)}{a(\phi)}$
$$
\frac{-b^{\prime \prime}(\theta)}{a(\phi)} \ 
\int_y f \ dy
+ \int_y  \frac{(y -b^{\prime}(\theta))^2}{a(\phi)^2} 
\ f \ dy = 0
$$

### Plug expected value $E(Y)$ from prior proof knowledge that $E(Y)$ = $b^{\prime}(\theta)$

$$
\frac{-b^{\prime \prime}(\theta)}{a(\phi)} 
+     \frac{1}{a(\phi)^2} \ 
\int_y (y - E(Y))^2
\ f \ dy = 0
$$

### Subsititute Variance $\sigma^2$ for $(y - E(Y))^2$
$$
\frac{-b^{\prime \prime}(\theta)}{a(\phi)} 
+     \frac{1}{a(\phi)^2} \ 
\sigma^2 = 0
$$


### Simplify by Multiplying by $a(\phi)$
$$
-b^{\prime \prime}(\theta) \ a(\phi) + \sigma^2 = 0
$$

### *Solution*: Therefore, we get see that the variance $\sigma^2$ is $b^{\prime \prime}a(\phi)$
$$
V(Y) = \sigma^2 = b^{\prime \prime}a(\phi)
$$

---

<br>

## 5. c) Show that the `Binomial` distribution belongs to the `exponential` family

### Given the binomial definition, rearrange:

$$
\begin{aligned}
p(Y = y) &=\left(\begin{array}{c}
n \\ y
\end{array}\right) p^{y}(1-p)^{n-y} \\
&=\left(\begin{array}{c}
n \\ y
\end{array}\right)(1-p)^{n}\left(\frac{p}{1-p}\right)^{y} \\
\end{aligned}
$$

### Therefore we see it is apart of the exponential family
$$
p(Y = y) = 
\left(\begin{array}{l} n \\ y
\end{array}\right)(1-p)^{n} \exp \left(\log \left(\frac{p}{1-p}\right) y\right)
$$

---

<br>

## 5. d) Show $E(Y) = np$ 

### Using the definition of a Moment Generating Function
$$
b(y ; n, p)=\frac{n !}{y !(n-y) !} p^{y} q^{n-y} \quad \text { with } \quad q=1-p .
$$

### MGF Given by:
$$
\begin{aligned}
M(y, t) &=\sum_{y=0}^{n} e^{y t} \frac{n !}{y !(n-y) !} p^{y} q^{n-y} \\
&=\sum_{y=0}^{n} \frac{n !}{y !(n-y) !}\left(p e^{t}\right)^{y} q^{n-y} \\
&=\left(p e^{t}+q\right)^{n}
\end{aligned}
$$

### Differentiate the MGF with respect to $t$ using the function-of-a-function rule:
$$
\begin{aligned}
\frac{d M(y, t)}{d t} &=n\left(q+p e^{t}\right)^{n-1} p e^{t} \\
&=n p e^{t}\left(p e^{t}+q\right)^{n-1}
\end{aligned}
$$

### *Solution*: Now use $t=0$ to get $E(Y)$, which is = $n p$
$$
\begin{aligned}
E(Y) &=n p(p+q)^{n-1} \\
&=n p 
\end{aligned}
$$

---

<br>

## 5. e) Show $V(Y) = np(1−p)$

### Find the second moment using product rule:
$$
\frac{d u v}{d y}=u \frac{d v}{d y}+v \frac{d u}{d y}
$$
$$
\begin{aligned}
\frac{d^{2} M(y, t)}{d t^{2}} &=n p e^{t}\left\{(n-1)\left(p e^{t}+q\right)^{n-2} p e^{t}\right\}+\left(p e^{t}+q\right)^{n-1}\left\{n p e^{t}\right\} \\
&=n p e^{t}\left(p e^{t}+q\right)^{n-2}\left\{(n-1) p e^{t}+\left(p e^{t}+q\right)\right\} \\
&=n p e^{t}\left(p e^{t}+q\right)^{n-2}\left\{q+n p e^{t}\right\} .
\end{aligned}
$$

### Use $t=0$ again:
$$
\begin{aligned}
E\left(y^{2}\right) &=n p(p+q)^{n-2}(n p+q) \\
&=n p(n p+q)
\end{aligned}
$$

### Derive $V(Y)=E(Y-\mu)^{2}$, in turn this means that $V(Y) = np(1 - p)$
> You could also write it as $V(Y) = n p q$ where $q=1-p$

$$
\begin{aligned}
E(Y) &=E\left(y^{2}\right)-\{E(Y)\}^{2} \\
&=n p(n p+q)-n^{2} p^{2} \\
&=n p q \\
\end{aligned}
$$

### *Solution*: Knowing that $q = 1 - p$, we know $V(Y) = n p (1 - p)$ from above $n p q$
$$ V(Y) = n p (1 - p)$$
---

<br>


# Question 6 - Derive Evidence

*Some of the derivation below may look similar to question 1 (a):* <br>  
*Adapted from JK's book - page 132 Doing Bayesian Data Analysis:* <br>  

If $\theta \sim \operatorname{Beta}(\alpha, \beta)$ and $X \sim \operatorname{Bin}(n, \theta)$, prove that $\theta \mid X \sim \operatorname{Beta}(x+\alpha, n-x+\beta)$ through proof below:    

$p(\theta \mid x) \propto p(\theta) p(x \mid \theta) = \frac{1}{B(\alpha, \beta)} \theta^{\alpha-1}(1-\theta)^{\beta-1}\left(\begin{array}{l}n \\ x\end{array}\right) \theta^{x}(1-\theta)^{n-x}$  


Note Bayes' rule  
$$
\underbrace{p(\theta \mid x)}_{Posterior} \propto 
\underbrace{p(\theta)}_{Prior} \underbrace{p(x \mid \theta)}_{Lik.} 
\ / \underbrace{p(x)}_{Evidence} 
= \frac{p(x, n \mid \theta) p(\theta)}{p(x, n)} / p(x) = p(\theta \mid x, n) / p(x)
$$ 

<br>  

Define Bernoulli and beta distributions  
$=\underbrace{\theta^{x}(1-\theta)^{(n-x)}}_{Bernoulli \ Lik.} \underbrace{\frac{\theta^{(\alpha-1)}(1-\theta)^{(\beta-1)}}{B(\alpha, \beta)}}_{Beta \ Prior} / p(x, n) / p(x)$ <br>  

Rearrange factors  
$=\frac{1}{B(\alpha, \beta) p(x, n)} \theta^{(\alpha-1)} (1-\theta)^{(\beta-1)} \theta^{x}(1-\theta)^{(n-x)} / p(x)$ <br>

$=\frac{1}{B(\alpha, \beta)} \theta^{(\alpha-1)} (1-\theta)^{(\beta-1)} \left(\begin{array}{l}\frac{1}{p(x, n)}\end{array}\right)\theta^{x}(1-\theta)^{(n-x)} / p(x)$ <br>

By definition of the binomial coefficient, which we arrive at the solution *in terms of `posterior`*:  
$$
\underbrace{p(\theta \mid x)}_{Posterior} = 
\frac{\frac{1}{B(\alpha, \beta)} \theta^{(\alpha-1)} (1-\theta)^{(\beta-1)} \left(\begin{array}{l}n \\ x\end{array}\right) \theta^{x}(1-\theta)^{(n-x)}} 
{\underbrace{p(x)}_{Evidence}}
$$ 

<br>

***Final Solution***: By rearranging to be *in terms of the `evidence`*
$$
\underbrace{p(x)}_{Evidence} = \frac{\frac{1}{B(\alpha, \beta)} \theta^{(\alpha-1)} (1-\theta)^{(\beta-1)} \left(\begin{array}{l}n \\ x\end{array}\right) \theta^{x}(1-\theta)^{(n-x)}} {\underbrace{p(\theta \mid x)}_{Posterior}}
$$ 

---

<br>

# Question 7 - 2 State MCMC

## 7. a) Find the value of `A`
* Since the die landed on 6, and 6 is $\notin$ `E1` (current acceptance set), then
* Reject the proposal of 2 and stay at state 1 
* Therefore, `A` = State `1`

## 7. b) Find the value of `B`
* Since we are at state `2` and the proposal state is state `1`, the value of alpha (or `B`) is `0.67`

## 7. c) Find the value of `C`
* Since the die landed on 6, and 6 is  $\in$ `E2` (current acceptance set), then
* Accept the proposal and change to at state 1 
* Therefore, `C` = State `1`

## 7. d) Formula for the acceptance probability
Note below is from page 33 of *Teaching MCMC* by Stewart & Stewart:

$$
\alpha_{i, j}=\min \left\{1, \frac{h_{j}}{h_{i}}\right\}
$$

where $\alpha_{i, j}$ is the probability of accepting state $j$ given that the sampler is at state $i$. 

---

<br>


# Question 8


## 8. a)
## 8. b)
## 8. c)
## 8. d)

---

<br>


# Question 9

```{r data9}
dataList = list(x = c(-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 
3, 4, 5, 6, 7, 8, 9, 10), y = c(21.72, 20.41, 14.06, 14.6, 13.8, 
10.4, 8.22, 6.79, 4.92, 0.45, 4.57, 1.95, 10.14, 13.28, 21.88, 
18.88, 22.91, 23.53, 34.99, 35.07, 39.17), N = 21)
```

## 9. b)
### 9. b,i)
### 9. b,ii)
### 9. b,iii)

## 9. c)
## 9. d)
## 9. e)

---

<br>


# Question 10

```{r}
x=50:80
set.seed(24)
y=10+2*x+rnorm(31,0,10)

xx=-15:15
set.seed(24)
yy=10+2*xx+rnorm(31,0,10)

```

## 10. a)
## 10. b)
### 10. b, i)
### 10. b, ii)



