---
title:    "Final Exam"
subtitle: "Bayesian Statistics"
author:   "Daniel Carpenter"
date:     "May 9, 2022"
fontsize: 12pt
geometry: margin=1in
output:
  html_document:
    toc: yes
    toc_float: yes
    # number_sections: yes
    toc_depth: 2
  # github_document:
  #   toc: yes
  #   number_sections: yes
  #   toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo    = TRUE,
	include = TRUE,
	message = FALSE,
	warning = FALSE,
	cache   = TRUE
)
```


Please place all answers in this document - use $\LaTeX$, R and Jags as needed. Answers to many qestions will be made dynamically through an R chunk.

# Question 1 - 

## 1. (a)
## 1. (b)
## 1. (c)
## 1. (d)
## 1. (e)
## 1. (f)
### 1. f, i)
### 1. f, ii)

---

<br>

# Question 2

## 2. a)
## 2. b)
## 2. c)
## 2. d)
## 2. e)

---

<br>

# Question 3

## 3. a)
## 3. b) 
## 3. c)
## 3. d)
## 3. e)

---

<br>


# Question 4

## 4. a)
## 4. b) 
## 4. c)
## 4. d)
### 4. d, 1)
### 4. d, 2)

---

<br>


# Question 5 - Exponential Family

The exponential family can be defined as any density of the following form:
$$
f(y \mid \theta, \phi)=\exp \left(\frac{y \theta-b(\theta)}{a(\phi)}+c(y, \phi)\right)
$$
<br>

## 5. a) Show $E(Y) = b^{\prime}(\theta)$

### Sum of distribution is 1
$$
\int_y f(y)dy = 1 \\
$$

### Differentiate above equation
$$
\frac{d}{d\theta} \int_y f(y)dy = \frac{d}{d\theta} 1 = 0
$$

### Take inside the integral
$$
\int_y \frac{df}{d\theta} dy = 0
$$

### Differentiate with $f$ being the exponential
$$
\frac{df}{d\theta} = 
\frac{y -b^{\prime}(\theta)}{a(\phi)} \ 
\exp \left(\frac{y \theta-b(\theta)}{a(\phi)}
+ c(y, \phi)\right)
$$

### Replace with $f$ since it is the expontial
$$
\frac{df}{d\theta} = 
\frac{y -b^{\prime}(\theta)}{a(\phi)} \ f
$$

### Plug $\int_y \frac{df}{d\theta} dy = 0$ into above $\frac{df}{d\theta}$
$$
\int_y \begin{bmatrix} \frac{y -b^{\prime}(\theta)}{a(\phi)} \end{bmatrix} \ f \ dy = 0
$$

### Distribute $f$
$$
\int_y \begin{bmatrix} \frac{yf -b^{\prime}(\theta)f}{a(\phi)} \end{bmatrix} \ f \ dy = 0
$$

### Multiply by $a(\phi)$ to get rid of it
$$
\int_y yf \ dy 
- \int_y b^{\prime}(\theta)f \ dy= 0
$$

### Writing above in terms of E(Y)
$$
E(Y) - b^{\prime}(\theta) \ \int_yf \ dy= 0
$$

### *Solution*: Therefore, the expected value of Y $E(Y)$ is $b^{\prime}(\theta)$
$$
E(Y) = b^{\prime}(\theta)
$$

---

<br>


## 5. b) Show $V(Y) = b^{\prime \prime}(\theta) a(\phi)$

### From the prior proof above
$$
\frac{df}{d\theta} = 
\begin{bmatrix} \frac{y -b^{\prime}(\theta)}{a(\phi)} \end{bmatrix} \ f
$$

### Get the second derivative of above
$$
\frac{d^2f}{d\theta^2} = 
\frac{-b^{\prime \prime}(\theta)}{a(\phi)} \ f
+ \frac{y -b^{\prime}(\theta)}{a(\phi)} \ f^{\prime}
$$

### Replace above $f^{\prime}$ with $f$ from prior proof (two above points)
$$
\frac{d^2f}{d\theta^2} = 
\frac{-b^{\prime \prime}(\theta)}{a(\phi)} \ f
+ \begin{bmatrix} \frac{y -b^{\prime}(\theta)}{a(\phi)} \end{bmatrix}^2 \ f
$$

### Note sum is 1 for all densities
$$
\int_y dy = 1 \\
$$

### Take 2nd dertivative using above notes to get 0
$$
\int_y \frac{d^2f}{d\theta^2} \ dy = 0
$$

### Plug $\frac{d^2f}{d\theta^2}$ into above equation
$$
\int_y
\begin{pmatrix}
\frac{-b^{\prime \prime}(\theta)}{a(\phi)} \ f
+ \begin{bmatrix} \frac{y -b^{\prime}(\theta)}{a(\phi)} \end{bmatrix}^2 \ f
\end{pmatrix} \
dy = 0
$$

### Simplify prior equation and isolate $\frac{-b^{\prime \prime}(\theta)}{a(\phi)}$
$$
\frac{-b^{\prime \prime}(\theta)}{a(\phi)} \ 
\int_y f \ dy
+ \int_y  \frac{(y -b^{\prime}(\theta))^2}{a(\phi)^2} 
\ f \ dy = 0
$$

### Plug expected value $E(Y)$ from prior proof knowledge that $E(Y)$ = $b^{\prime}(\theta)$

$$
\frac{-b^{\prime \prime}(\theta)}{a(\phi)} 
+     \frac{1}{a(\phi)^2} \ 
\int_y (y - E(Y))^2
\ f \ dy = 0
$$

### Subsititute Variance $\sigma^2$ for $(y - E(Y))^2$
$$
\frac{-b^{\prime \prime}(\theta)}{a(\phi)} 
+     \frac{1}{a(\phi)^2} \ 
\sigma^2 = 0
$$


### Simplify by Multiplying by $a(\phi)$
$$
-b^{\prime \prime}(\theta) \ a(\phi) + \sigma^2 = 0
$$

### *Solution*: Therefore, we get see that the variance $\sigma^2$ is $b^{\prime \prime}a(\phi)$
$$
V(Y) = \sigma^2 = b^{\prime \prime}a(\phi)
$$

---

<br>

## 5. c) Show that the `Binomial` distribution belongs to the `exponential` family

### Given the binomial definition, rearrange:

$$
\begin{aligned}
p(Y = y) &=\left(\begin{array}{c}
n \\ y
\end{array}\right) p^{y}(1-p)^{n-y} \\
&=\left(\begin{array}{c}
n \\ y
\end{array}\right)(1-p)^{n}\left(\frac{p}{1-p}\right)^{y} \\
\end{aligned}
$$

### Therefore we see it is apart of the exponential family
$$
p(Y = y) = 
\left(\begin{array}{l} n \\ y
\end{array}\right)(1-p)^{n} \exp \left(\log \left(\frac{p}{1-p}\right) y\right)
$$

---

<br>

## 5. d) Show $E(Y) = np$ 

### Using the definition of a Moment Generating Function
$$
b(y ; n, p)=\frac{n !}{y !(n-y) !} p^{y} q^{n-y} \quad \text { with } \quad q=1-p .
$$

### MGF Given by:
$$
\begin{aligned}
M(y, t) &=\sum_{y=0}^{n} e^{y t} \frac{n !}{y !(n-y) !} p^{y} q^{n-y} \\
&=\sum_{y=0}^{n} \frac{n !}{y !(n-y) !}\left(p e^{t}\right)^{y} q^{n-y} \\
&=\left(p e^{t}+q\right)^{n}
\end{aligned}
$$

### Differentiate the MGF with respect to $t$ using the function-of-a-function rule:
$$
\begin{aligned}
\frac{d M(y, t)}{d t} &=n\left(q+p e^{t}\right)^{n-1} p e^{t} \\
&=n p e^{t}\left(p e^{t}+q\right)^{n-1}
\end{aligned}
$$

### *Solution*: Now use $t=0$ to get $E(Y)$, which is = $n p$
$$
\begin{aligned}
E(Y) &=n p(p+q)^{n-1} \\
&=n p 
\end{aligned}
$$

---

<br>

## 5. e) Show $V(Y) = np(1âˆ’p)$

### Find the second moment using product rule:
$$
\frac{d u v}{d y}=u \frac{d v}{d y}+v \frac{d u}{d y}
$$
$$
\begin{aligned}
\frac{d^{2} M(y, t)}{d t^{2}} &=n p e^{t}\left\{(n-1)\left(p e^{t}+q\right)^{n-2} p e^{t}\right\}+\left(p e^{t}+q\right)^{n-1}\left\{n p e^{t}\right\} \\
&=n p e^{t}\left(p e^{t}+q\right)^{n-2}\left\{(n-1) p e^{t}+\left(p e^{t}+q\right)\right\} \\
&=n p e^{t}\left(p e^{t}+q\right)^{n-2}\left\{q+n p e^{t}\right\} .
\end{aligned}
$$

### Use $t=0$ again:
$$
\begin{aligned}
E\left(y^{2}\right) &=n p(p+q)^{n-2}(n p+q) \\
&=n p(n p+q)
\end{aligned}
$$

### Derive $V(Y)=E(Y-\mu)^{2}$, in turn this means that $V(Y) = np(1 - p)$
> You could also write it as $V(Y) = n p q$ where $q=1-p$

$$
\begin{aligned}
E(Y) &=E\left(y^{2}\right)-\{E(Y)\}^{2} \\
&=n p(n p+q)-n^{2} p^{2} \\
&=n p q \\
\end{aligned}
$$

### *Solution*: Knowing that $q = 1 - p$, we know $V(Y) = n p (1 - p)$ from above $n p q$
$$ V(Y) = n p (1 - p)$$
---

<br>


# Question 6

---

<br>

# Question 7

## 7. a)
## 7. b)
## 7. c)
## 7. d)

---

<br>


# Question 8


## 8. a)
## 8. b)
## 8. c)
## 8. d)

---

<br>


# Question 9

```{r data9}
dataList = list(x = c(-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 
3, 4, 5, 6, 7, 8, 9, 10), y = c(21.72, 20.41, 14.06, 14.6, 13.8, 
10.4, 8.22, 6.79, 4.92, 0.45, 4.57, 1.95, 10.14, 13.28, 21.88, 
18.88, 22.91, 23.53, 34.99, 35.07, 39.17), N = 21)
```

## 9. b)
### 9. b,i)
### 9. b,ii)
### 9. b,iii)

## 9. c)
## 9. d)
## 9. e)

---

<br>


# Question 10

```{r}
x=50:80
set.seed(24)
y=10+2*x+rnorm(31,0,10)

xx=-15:15
set.seed(24)
yy=10+2*xx+rnorm(31,0,10)

```

## 10. a)
## 10. b)
### 10. b, i)
### 10. b, ii)



