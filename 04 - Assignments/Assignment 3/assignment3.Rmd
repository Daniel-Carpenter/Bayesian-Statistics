---
title:    "Assignment 3"
subtitle: "Bayesian Statistics"
author:   "Daniel Carpenter"
date:     "April 2022"
fontsize: 12pt
geometry: margin=1in
output:
  html_document:
    toc: yes
    toc_float: yes
  # github_document:
  #   toc: yes
  #   # number_sections: yes
  #   toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

---

# Task 1

## `a`-`b`

```{r}
# The data input
y = c( rep(1,9),rep(0,3) , rep(1,45),rep(0,15) , rep(1,3),rep(0,9) ) 
s = c( rep("A",12) , rep("B",60) , rep("C",12) )
fileName = "Ass3.1.csv"
write.csv( data.frame(y=y,s=s) , file=fileName , row.names=FALSE )


# Example for Jags-Ydich-XnomSsubj-Mbernbeta.R 
#------------------------------------------------------------------------------- 
# Optional generic preliminaries:
graphics.off() # This closes all of R's graphics windows.

#------------------------------------------------------------------------------- 
# Load The data 
myData = read.csv(fileName)

# Include head of data
head(myData)

# N.B.: The functions below expect the data to be a data frame, 
# with one component named y being a vector of integer 0,1 values,
# and one component named s being a factor of subject identifiers.
myData$s = factor( myData$s )

#------------------------------------------------------------------------------- 
# Load the relevant model into R's working memory:
source("Jags-Ydich-XnomSsubj-MbernBeta.R")

#------------------------------------------------------------------------------- 
# Optional: Specify filename root and graphical format for saving output.
# Otherwise specify as NULL or leave saveName and saveType arguments 
# out of function calls.
dir.create('Output')
fileNameRoot = "Output//"
graphFileType = "png" 

#------------------------------------------------------------------------------- 
# Generate the MCMC chain:
mcmcCoda = genMCMC( data=myData , numSavedSteps=50000 , saveName=fileNameRoot )

#------------------------------------------------------------------------------- 
# Display diagnostics of chain, for specified parameters:
parameterNames = varnames(mcmcCoda) # get all parameter names
for ( parName in parameterNames ) {
  diagMCMC( codaObject=mcmcCoda , parName=parName , 
            saveName=fileNameRoot , saveType=graphFileType )
}

# Ensure that the graphs of theta[1] and [2] are shown
knitr::include_graphics(paste0(fileNameRoot, 'Diagtheta[1].', graphFileType), dpi = 5)
knitr::include_graphics(paste0(fileNameRoot, 'Diagtheta[2].', graphFileType), dpi = 5)

#------------------------------------------------------------------------------- 
# Get summary statistics of chain:
summaryInfo = smryMCMC( mcmcCoda , compVal=NULL , rope=c(0.45,0.55) ,
                        compValDiff=0.0 , ropeDiff = c(-0.05,0.05) ,
                        saveName=fileNameRoot )

# Display posterior information:
plotMCMC( mcmcCoda , data=myData , compVal=NULL , rope=c(0.45,0.55) ,
          compValDiff=0.0 , ropeDiff = c(-0.05,0.05) ,
          saveName=fileNameRoot , saveType=graphFileType )
```


## `c` Credibility of Model
* The MCMC estimation appears credible across all values of theta since:
  * Param. Values seem to be around the mean value, which is stationary
  * Shrink factor evens out
  * Autocorrelation drops quickly
  * Each chain's desnity represents each other well when viewing superimposed over each other

## `d` Sample Size Effect
* Increased sample size of subject B will cause for a tighter distribution on the estimation

```{r}
# Add the histogram
plotPost(mcmcCoda[,'theta[1]'], xlim = c(0,1), xlab= expression(theta[1]),
         main = 'Daniel Carpenter', col = 'darkseagreen3',
                      showCurve = FALSE)

# Add the density line
paramSampleVec = as.matrix(mcmcCoda[,'theta[1]'])
densCurve = density( paramSampleVec , adjust=2 )
a = densCurve$x; b = densCurve$y

lines(a, b, type="l" , lwd=2.5 , col='darkseagreen4' , bty="n")


graphics.off() # This closes all of R's graphics windows.
```

## `e`-`f` Lines()
* `a` and `b` resemble come from the `density()` function from the `mcmcCoda[,'theta[1]']` object
* The lines command adds a line to a plot as a layer.

## `h` Functions
i. `xlab= expression(theta[1])`
ii. `col = 'green3'`; note I used `col = 'darkseagreen3'`
iii. `main = 'Daniel Carpenter'`

## `i` Lines()
* You can use `showCurve = TRUE` but it does not plot both the histogram and the line, so I manually added the line.



---

<br>

# Task 2 Shrinkage

## `a` Latex Explanation
* Understand shrinkage (consquence to heirarchical model).
* Use the MLE approach
* Goal is to find the values of the parameters in the formula that will maximize the probability of the data.

$$
\begin{aligned}
&p\left(\left\{y_{i \mid s}\right\} \mid\left\{\theta_{s}\right\}, \omega, \kappa\right) \\
&=\prod_{s} \prod_{i \mid s} p\left(y_{i \mid s} \mid \theta_{s}, \omega, \kappa\right) \\
&=\prod_{s} \prod_{i \mid s} \operatorname{bern}\left(y_{i \mid s} \mid \theta_{s}\right) \cdot \operatorname{beta}\left(\theta_{s} \mid \omega(\kappa-2)+1,(1-\omega)(\kappa-2)+1\right)
\end{aligned}
$$

## `b` Proportions of Heads
```{r propHead}
y_s1 = c(1,0,0,0); paste('Proportion of Heads for y[s[1]]:', sum(y_s1) / sum(y_s1>=0)) # for y_s1
y_s2 = c(1,1,0,0); paste('Proportion of Heads for y[s[2]]:', sum(y_s2) / sum(y_s2>=0)) # for y_s2
y_s3 = c(1,1,0,0); paste('Proportion of Heads for y[s[3]]:', sum(y_s3) / sum(y_s3>=0)) # for y_s3
y_s4 = c(1,1,1,0); paste('Proportion of Heads for y[s[4]]:', sum(y_s4) / sum(y_s4>=0)) # for y_s4
```

## `c-e` Shrinkage Param. Set 1
```{r likFun}
# Function to estimate the liklihood from the MLE
likFromMLE <- function(N, z, omega, kappa, theta) {
  alpha = omega * (kappa-2) + 1 
  beta = (1-omega) * (kappa-2) + 1
  mleProb = theta^z * (1-theta)^(N-z) * dbeta(theta, alpha, beta)
  lik = prod(mleProb) 
  
  # Create a plot
  plot(x=seq(1/length(theta),1,1/length(theta)), y=dbeta(theta, alpha, beta), 
       main = 'Beta in Likelihood Function - Daniel Carpenter', 
       xlab=expression(theta),
       type = "b", pch = 19, col = 'steelblue3')
  
  paste('Likelihood Estimate:', lik)
  return(lik) # Return the likelihood
}
```
> Likelihood when 𝜔=0.5, 𝜅=2, 𝜃1=0.25, 𝜃2=0.50, 𝜃3=0.50, and 𝜃4=0.75?  

* `c` See output below  
* `d` These parameters do not since it is uniform  
* `e` The shape is flat (see output below)  

```{r 2c_e}
# Inputs 
N     = rep(4,4) 
omega = 0.5 
kappa = 2 
theta = c(0.25, 0.50, 0.50, 0.75) 
z = sum(y_s1, y_s2, y_s3, y_s4) # Sum of the heads

# Call the function
likFromMLE(N, z, omega, kappa, theta)
```

## `f`-`i` Shrinkage Param. Set 2
> Likelihood when 𝜔=0.5, 𝜅=2, 𝜃1=0.35, 𝜃2=0.50, 𝜃3=0.50, and 𝜃4=0.65?  

* `c` See output below  
* `d` These parameters do increase the likelihood (see the summary distribution below compared to the previous problem)
* `e` The shape is curved (see output below)  

```{r 2f_i}
# Inputs 
N     = rep(4,4) 
omega = 0.5 
kappa = 20 
theta = c(0.35, 0.50, 0.50, 0.65) 
z = sum(y_s1, y_s2, y_s3, y_s4) # Sum of the heads

# Call the function
likFromMLE(N, z, omega, kappa, theta)
```

## `j` Shrinkage affect on Liklihood
> What does shrinkage do to likelihood?

* Shrinkage occurs because the estimate of each low-level parameter is influenced from two
sources:
1. the subset of data that are directly dependent on the low-level parameter, and
2. the higher-level parameters on which the low-level parameter depends.

* Shrinkage improves the likelihood estimate drastically if the parameters in the formula do so.


---

<br>

# Task 3 Fair Coins

## `a` What is the two-tailed p value?  
The two-tailed p-value is simply two times the one-tailed `p` value. See one-tailed description in part `c`

### Fix `N`:  
$$
p(z \mid N, \theta)=
\left(\begin{array}{c}  N \\ z  \end{array}\right) 
\theta^{z}(1-\theta)^{N-z}
$$


```{r 1a}
N = 45 ; z = 3 ; theta = 1/6 

lowTailZ = 0:z
paste('Two tailed p-value:', 2 * sum( choose(N,lowTailZ) * theta^lowTailZ * (1-theta)^(N-lowTailZ) ) )
```


## `b` Explain each line of the script (one-tail)
```{r 1b, eval=FALSE}
N = 45 ; z = 3 ; theta = 1/6 # Inputs as defined in problem 3 description

lowTailZ = 0:z # Create a vector from 0 to z (3) to represent the successes

# Calculates the probability of getting z success out of N trials is
sum( choose(N,lowTailZ) * theta^lowTailZ * (1-theta)^(N-lowTailZ) )
```


## `c` Why does it consider the low tail and not the high tail?
* Since $(z / N)_{\text {actual }} < E\left[(z / N)_{\theta, I}\right]$, we use lower tail. 
* If $(z / N)_{\text {actual }} > E\left[(z / N)_{\theta, I}\right]$, then we would use the upper tail.

## `d` Explain the meaning of the final result.
* The final results gives the probability `p` of getting `z` heads out of `N` flips
* I.e., the probability `p` of getting `z` heads out of `N` flips is 0.04460167
* Above result is for a one tailed experiment

## `e`-`f`

### Explain the code
```{r 1e}
sum( (lowTailZ/N) * choose(N,lowTailZ) * theta^lowTailZ * (1-theta)^(N-lowTailZ) )
```

### Fix `z`:  
$$
\begin{aligned}
p(N \mid z, \theta) &=\left(\begin{array}{c}
N-1 \\
z-1
\end{array}\right) \theta^{z-1}(1-\theta)^{N-z} \cdot \theta \\
&=\left(\begin{array}{c}
N-1 \\
z-1
\end{array}\right) \theta^{z}(1-\theta)^{N-z} \\
&=\frac{z}{N}\left(\begin{array}{c}
N \\
z
\end{array}\right) \theta^{z}(1-\theta)^{N-z}
\end{aligned}
$$

* `z` is fixed in advance and `N` is the random variable. 
* Interpret as the probability of taking `N` flips to get `z` heads

### Explain the results
the probability `p` of getting `z` heads out of `N` flips is 0.002605228


---

<br>


# Task 4 Dicotomous Outcome
> Continuing from `Task 3` with a dichotomous outcome  with `N` = 45 and `z` = 3. 

## `a` Stop when `N` = 45. What is the 95% CI?
```{r 4a}
N = 45 # Number of Trials
z = 3  # Number of Successes

# Low Tail - iterate over values of theta from 0.170 to 0.190 in increments of 0.001
for ( theta in seq( 0.170 , 0.190 , 0.001) ) {
  
  show( # Show the list object for each value of theta
    c( # Create a list including value of theta and the 2-tail probability
      
      # Get the two-tail probability using the LOWER tail estimate. Multuply by 2 for 2 tail
      # Interpret as the probability p of getting z heads for a given value of theta
      theta , 2*sum( choose(N,lowTailZ) * theta^lowTailZ * (1-theta)^(N-lowTailZ))
    )
  ) 
}

highTailZ = z:N 

# High Tail - iterate over values of theta from 0.005 to 0.020 in increments of 0.001
for ( theta in seq( 0.005 , 0.020 , 0.001) ) { 
  
  show( # Show the list object for each value of theta
    c( # Create a list including value of theta and the 2-tail probability
      
      # Get the two-tail probability using the UPPER tail estimate. Multuply by 2 for 2 tail
      # Interpret as the probability p of getting z heads for a given value of theta
      theta , 2*sum( choose(N,highTailZ) * theta^highTailZ * (1-theta)^(N-highTailZ) ) 
    )
  ) 
}
```

* We can be 95% confident that values of theta will fall between $0.0130$ and $0.1830$

## `b` Explain above code
* See comments above for details and interpretation
* 
## `c` Stop when `z` = 3, what is the 95% CI?
* Note altered the code by adding `(lowTailZ/N) *` and `(highTailZ/N) *`
```{r 4c}
# Low Tail - iterate over values of theta from 0.170 to 0.190 in increments of 0.001
for ( theta in seq( 0.170 , 0.190 , 0.001) ) {
  
  show( # Show the list object for each value of theta
    c( # Create a list including value of theta and the 2-tail probability
      
      # Get the two-tail probability using the LOWER tail estimate. Multuply by 2 for 2 tail
      # Interpret as the probability p of getting z heads for a given value of theta
      theta , 2*sum( (lowTailZ/N) * choose(N,lowTailZ) * theta^lowTailZ * (1-theta)^(N-lowTailZ))
      #               ^^^^^^^^^^^^^
    )
  ) 
}

highTailZ = z:N 

# High Tail - iterate over values of theta from 0.005 to 0.020 in increments of 0.001
for ( theta in seq( 0.005 , 0.020 , 0.001) ) { 
  
  show( # Show the list object for each value of theta
    c( # Create a list including value of theta and the 2-tail probability
      
      # Get the two-tail probability using the UPPER tail estimate. Multuply by 2 for 2 tail
      # Interpret as the probability p of getting z heads for a given value of theta
      theta , 2*sum( (highTailZ/N) * choose(N,highTailZ) * theta^highTailZ * (1-theta)^(N-highTailZ) ) 
      #               ^^^^^^^^^^^^^^
    )
  ) 
}
```

## `d` Is the CI the same as for stopping when N = 45? 
* No it is not. 
* All values shown fall within the 95% confidence interval. I.e., $\forall \ p \leq 0.05$


