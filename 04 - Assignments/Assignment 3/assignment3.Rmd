---
title:    "Assignment 3"
subtitle: "Bayesian Statistics"
author:   "Daniel Carpenter"
date:     "April 2022"
fontsize: 12pt
geometry: margin=1in
output:
  html_document:
    toc: yes
    toc_float: yes
  # github_document:
  #   toc: yes
  #   # number_sections: yes
  #   toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message=FALSE,
	warning=FALSE
)
```

---

# Task 1

## `a`-`b`

```{r}
# The data input
y = c( rep(1,9),rep(0,3), rep(1,45),rep(0,15), rep(1,3),rep(0,9) ) 
s = c( rep("A",12), rep("B",60), rep("C",12) )
fileName = "Ass3.1.csv"
write.csv( data.frame(y=y,s=s), file=fileName, row.names=FALSE )


# Example for Jags-Ydich-XnomSsubj-Mbernbeta.R 
#------------------------------------------------------------------------------- 
# Optional generic preliminaries:
graphics.off() # This closes all of R's graphics windows.

#------------------------------------------------------------------------------- 
# Load The data 
myData = read.csv(fileName)

# Include head of data
head(myData)

# N.B.: The functions below expect the data to be a data frame, 
# with one component named y being a vector of integer 0,1 values,
# and one component named s being a factor of subject identifiers.
myData$s = factor( myData$s )

#------------------------------------------------------------------------------- 
# Load the relevant model into R's working memory:
source("Jags-Ydich-XnomSsubj-MbernBeta.R")

#------------------------------------------------------------------------------- 
# Optional: Specify filename root and graphical format for saving output.
# Otherwise specify as NULL or leave saveName and saveType arguments 
# out of function calls.
dir.create('Output')
fileNameRoot = "Output//"
graphFileType = "png" 

#------------------------------------------------------------------------------- 
# Generate the MCMC chain:
mcmcCoda = genMCMC( data=myData, numSavedSteps=50000, saveName=fileNameRoot )

#------------------------------------------------------------------------------- 
# Display diagnostics of chain, for specified parameters:
parameterNames = varnames(mcmcCoda) # get all parameter names
for ( parName in parameterNames ) {
  diagMCMC( codaObject=mcmcCoda, parName=parName, 
            saveName=fileNameRoot, saveType=graphFileType )
}

# Ensure that the graphs of theta[1] and [2] are shown
knitr::include_graphics(paste0(fileNameRoot, 'Diagtheta[1].', graphFileType), dpi = 5)
knitr::include_graphics(paste0(fileNameRoot, 'Diagtheta[2].', graphFileType), dpi = 5)

#------------------------------------------------------------------------------- 
# Get summary statistics of chain:
summaryInfo = smryMCMC( mcmcCoda, compVal=NULL, rope=c(0.45,0.55),
                        compValDiff=0.0, ropeDiff = c(-0.05,0.05),
                        saveName=fileNameRoot )

# Display posterior information:
plotMCMC( mcmcCoda, data=myData, compVal=NULL, rope=c(0.45,0.55),
          compValDiff=0.0, ropeDiff = c(-0.05,0.05),
          saveName=fileNameRoot, saveType=graphFileType )
```


## `c` Credibility of Model
* The MCMC estimation appears credible across all values of theta since:
  * Param. Values seem to be around the mean value, which is stationary
  * Shrink factor evens out
  * Autocorrelation drops quickly
  * Each chain's desnity represents each other well when viewing superimposed over each other

## `d` Sample Size Effect
* Increased sample size of subject B will cause for a tighter distribution on the estimation

```{r}
# Add the histogram
plotPost(mcmcCoda[,'theta[1]'], xlim = c(0,1), xlab= expression(theta[1]),
         main = 'Daniel Carpenter', col = 'darkseagreen3',
                      showCurve=FALSE)

# Add the density line
paramSampleVec = as.matrix(mcmcCoda[,'theta[1]'])
densCurve = density( paramSampleVec, adjust=2 )
a = densCurve$x; b = densCurve$y

lines(a, b, type="l", lwd=2.5, col='darkseagreen4', bty="n")


graphics.off() # This closes all of R's graphics windows.
```

## `e`-`f` Lines()
* `a` and `b` resemble come from the `density()` function from the `mcmcCoda[,'theta[1]']` object
* The lines command adds a line to a plot as a layer.

## `h` Functions
i. `xlab= expression(theta[1])`
ii. `col = 'green3'`; note I used `col = 'darkseagreen3'`
iii. `main = 'Daniel Carpenter'`

## `i` Lines()
* You can use `showCurve = TRUE` but it does not plot both the histogram and the line, so I manually added the line.



---

<br>

# Task 2 Shrinkage

## `a` Latex Explanation
* Understand shrinkage (consquence to heirarchical model).
* Use the MLE approach
* Goal is to find the values of the parameters in the formula that will maximize the probability of the data.

$$
\begin{aligned}
&p\left(\left\{y_{i \mid s}\right\} \mid\left\{\theta_{s}\right\}, \omega, \kappa\right) \\
&=\prod_{s} \prod_{i \mid s} p\left(y_{i \mid s} \mid \theta_{s}, \omega, \kappa\right) \\
&=\prod_{s} \prod_{i \mid s} \operatorname{bern}\left(y_{i \mid s} \mid \theta_{s}\right) \cdot \operatorname{beta}\left(\theta_{s} \mid \omega(\kappa-2)+1,(1-\omega)(\kappa-2)+1\right)
\end{aligned}
$$

## `b` Proportions of Heads
```{r propHead}
y_s1 = c(1,0,0,0); paste('Proportion of Heads for y[s[1]]:', sum(y_s1) / sum(y_s1>=0)) # for y_s1
y_s2 = c(1,1,0,0); paste('Proportion of Heads for y[s[2]]:', sum(y_s2) / sum(y_s2>=0)) # for y_s2
y_s3 = c(1,1,0,0); paste('Proportion of Heads for y[s[3]]:', sum(y_s3) / sum(y_s3>=0)) # for y_s3
y_s4 = c(1,1,1,0); paste('Proportion of Heads for y[s[4]]:', sum(y_s4) / sum(y_s4>=0)) # for y_s4
```

## `c-e` Shrinkage Param. Set 1
```{r likFun}
# Function to estimate the liklihood from the MLE
likFromMLE <- function(N, z, omega, kappa, theta) {
  alpha = omega * (kappa-2) + 1 
  beta = (1-omega) * (kappa-2) + 1
  mleProb = theta^z * (1-theta)^(N-z) * dbeta(theta, alpha, beta)
  lik = prod(mleProb) 
  
  # Create a plot
  plot(x=seq(1/length(theta),1,1/length(theta)), y=dbeta(theta, alpha, beta), 
       main = 'Beta in Likelihood Function - Daniel Carpenter', 
       xlab=expression(theta),
       type = "b", pch = 19, col = 'steelblue3')
  
  paste('Likelihood Estimate:', lik)
  return(lik) # Return the likelihood
}
```
> Likelihood when 𝜔=0.5, 𝜅=2, 𝜃1=0.25, 𝜃2=0.50, 𝜃3=0.50, and 𝜃4=0.75?  

* `c` See output below  
* `d` These parameters do not since it is uniform  
* `e` The shape is flat (see output below)  

```{r 2c_e}
# Inputs 
N     = rep(4,4) 
omega = 0.5 
kappa = 2 
theta = c(0.25, 0.50, 0.50, 0.75) 
z = sum(y_s1, y_s2, y_s3, y_s4) # Sum of the heads

# Call the function
likFromMLE(N, z, omega, kappa, theta)
```

## `f`-`i` Shrinkage Param. Set 2
> Likelihood when 𝜔=0.5, 𝜅=2, 𝜃1=0.35, 𝜃2=0.50, 𝜃3=0.50, and 𝜃4=0.65?  

* `c` See output below  
* `d` These parameters do increase the likelihood (see the summary distribution below compared to the previous problem)
* `e` The shape is curved (see output below)  

```{r 2f_i}
# Inputs 
N     = rep(4,4) 
omega = 0.5 
kappa = 20 
theta = c(0.35, 0.50, 0.50, 0.65) 
z = sum(y_s1, y_s2, y_s3, y_s4) # Sum of the heads

# Call the function
likFromMLE(N, z, omega, kappa, theta)
```

## `j` Shrinkage affect on Liklihood
> What does shrinkage do to likelihood?

* Shrinkage occurs because the estimate of each low-level parameter is influenced from two
sources:
1. the subset of data that are directly dependent on the low-level parameter, and
2. the higher-level parameters on which the low-level parameter depends.

* Shrinkage improves the likelihood estimate drastically if the parameters in the formula do so.


---

<br>

# Task 3 Fair Coins

## `a-b` Two-tailed p value and Code Expanation?  
Generally, the two-tailed p-value is simply two times the one-tailed `p` value. See one-tailed description in part `c`

### Fix `N`:  
$$
p(z \mid N, \theta)=
\left(\begin{array}{c}  N \\ z  \end{array}\right) 
\theta^{z}(1-\theta)^{N-z}
$$


```{r 1a}
N = 45 ; z = 3 ; theta = 1/6  # Data as defined in problem 3 description

lowTailZ = 0:z # Consider the low tail because z/N = 3/45 is less than expected p=1/6:

# Calculates the cumulative lower tail probability of getting z success out of N trials is
# Multiply by two to get the two-tail p value
paste('Two tailed p-value:', 2 * sum( choose(N,lowTailZ) * theta^lowTailZ * (1-theta)^(N-lowTailZ) ) )
```


## `c` Why does it consider the low tail and not the high tail?
* Since $(z / N)_{\text {actual }} < E\left[(z / N)_{\theta, I}\right]$, we use lower tail. 
* If $(z / N)_{\text {actual }} > E\left[(z / N)_{\theta, I}\right]$, then we would use the upper tail.
* E.g., $(z/N = 3/45) \leq (p=1/6)$


## `d` Explain the meaning of the final result.
* The final results gives the probability `p` of getting `z` heads out of `N` flips
* Two-tailed p-value is $\geq 0.05$, so we should not reject the hypothesis that $\theta = 1/6$

## `e`-`f`

### Explain the code
```{r 1e}
sum( (lowTailZ/N) * choose(N,lowTailZ) * theta^lowTailZ * (1-theta)^(N-lowTailZ) )


# The cumulative lower tail probability is a negative binomial, 
# however, the tail is over n >= N, so we compute as  1-p(n<N)
complN = z:(N-1)
complP = sum( (z/complN) * choose(complN,z) * theta^z * (1-theta)^(complN-z) )
lowTailP = 1-complP

# Two-tail probability:
TwoTailP = 2 * lowTailP
show( TwoTailP )
```

### Explain the results
* two-tailed p value is $< 0.05$, so reject the hypothesis that $\theta = 1/6$. 
* Difference in part A due to different stopping intention when establishing the sampling distribution.

---

<br>


# Task 4 Dicotomous Outcome
> Continuing from `Task 3` with a dichotomous outcome  with `N` = 45 and `z` = 3. 

## `a` Stop when `N` = 45. What is the 95% CI?
```{r 4a}
N = 45 # Number of Trials
z = 3  # Number of Successes

# Low Tail - iterate over values of theta from 0.170 to 0.190 in increments of 0.001
for ( theta in seq( 0.170, 0.190, 0.001) ) {
  
  show( # Show the list object for each value of theta
    c( # Create a list including value of theta and the 2-tail probability
      
      # Get the two-tail probability using the LOWER tail estimate. Multuply by 2 for 2 tail
      # Interpret as the probability p of getting z heads for a given value of theta
      theta, 2*sum( choose(N,lowTailZ) * theta^lowTailZ * (1-theta)^(N-lowTailZ))
    )
  ) 
}

highTailZ = z:N 

# High Tail - iterate over values of theta from 0.005 to 0.020 in increments of 0.001
for ( theta in seq( 0.005, 0.020, 0.001) ) { 
  
  show( # Show the list object for each value of theta
    c( # Create a list including value of theta and the 2-tail probability
      
      # Get the two-tail probability using the UPPER tail estimate. Multuply by 2 for 2 tail
      # Interpret as the probability p of getting z heads for a given value of theta
      theta, 2*sum( choose(N,highTailZ) * theta^highTailZ * (1-theta)^(N-highTailZ) ) 
    )
  ) 
}
```

* We can be 95% confident that values of theta will fall between $0.014$ and $0.182$

## `b` Explain above code
* See comments above for details and interpretation
* 
## `c` Stop when `z` = 3, what is the 95% CI?
* Note altered the code by adding `(lowTailZ/N) *` and `(highTailZ/N) *`
```{r 4c}
# For candidate theta values GREATER than z/N observed, compute the LEFT-tail p-value:
complN = z:(N-1)

# Low Tail - iterate over values of theta
for ( theta in seq( 0.150, 0.160, 0.001) ) {
  
  show( # Show the list object for each value of theta
    c( # Create a list including value of theta and the 2-tail probability
      
      # Get the two-tail probability using the LOWER tail estimate. Multuply by 2 for 2 tail
      # Interpret as the probability p of getting z heads for a given value of theta
      theta, 2*(1-sum( (z/complN) * choose(complN,z) * theta^z * (1-theta)^(complN-z) ) )

    )
  ) 
}

# For candidate theta values LESS than z/N observed, compute the RIGHT-tail p-value:
highTailN = z:N # Notice N not N-1

# High Tail - iterate over values of theta from 0.005 to 0.020 in increments of 0.001
for ( theta in seq( 0.005, 0.020, 0.001) ) { 
  
  show( # Show the list object for each value of theta
    c( # Create a list including value of theta and the 2-tail probability
      
      # Get the two-tail probability using the UPPER tail estimate. Multuply by 2 for 2 tail
      # Interpret as the probability p of getting z heads for a given value of theta
      theta, 2*sum((z/highTailN) * choose(highTailN,z) * theta^z * (1-theta)^(highTailN-z)) 
    )
  ) 
}
```

## `d` Is the CI the same as for stopping when N = 45? 
* When the stopping at fixed z, the confidence interval is from $0.014$ to $0.154$

---

<br>

# Task 5

## `a` Explain Graph

* Overview: start with the actual or idealized data and then use Bayes’ rule to generate the corresponding distribution on parameter values

1. Real or hypothetical world creates an actual or an idealized data set
2. Bayes’ rule is the applied, which creates a posterior distribution
3. Posterior distribution is used as the hypothetical distribution of parameter values for power analysis

**Interpret code:**  

* Some `#` comments copied and pasted, ***but added own comments to ensure intepretation is right***

## `b`
```{r, eval=TRUE, cache=TRUE}
# Get key functions: genMCMC, smryMCMC, and plotMCMC, and get DBDA2E-utilities.R
source("Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa.R")

# The idealized hypothesis. This could be anecdotal evidence
idealGroupMean = 0.65
idealGroupSD   = 0.07

# Specify how much (idealized) data we have to support that hypothesis. 
# The more data, the more confidence in the hypothesis
idealNsubj       = 100  # more subjects => higher confidence in hypothesis
idealNtrlPerSubj = 100  # more trials => higher confidence in hypothesis
```

## `c`
```{r, eval=TRUE, cache=TRUE}
# goal is to generate data that supports the claims above:

# Generate random theta values for idealized subjects:
betaAB = betaABfromMeanSD( idealGroupMean, idealGroupSD )
theta = rbeta( idealNsubj, betaAB$a, betaAB$b )
```

## `d`
```{r, eval=TRUE, cache=TRUE}
# Transform the theta values to exactly match idealized mean, SD:
theta = ((theta-mean(theta))/sd(theta))*idealGroupSD + idealGroupMean
theta[ theta >= 0.999 ] = 0.999 # must be between 0 and 1
theta[ theta <= 0.001 ] = 0.001 # must be between 0 and 1

# Generate idealized data very close to theta’s:
z = round( theta*idealNtrlPerSubj )
```

## `e`
```{r, eval=TRUE, cache=TRUE}
# Convert to data format needed by JAGS function:
# Set up an empty matrix for holding the data:
dataMat=matrix(0,ncol=2,nrow=0,dimnames=list(NULL,c("y","s")))

# For each simulated subject,
for ( sIdx in 1:idealNsubj ) {
  
  # Create vector of 0’s and 1’s matching the z values generated above:
  yVec = c(rep(1,z[sIdx]),rep(0,idealNtrlPerSubj-z[sIdx]))
  
  # Bind the subject data to the bottom of the matrix:
  dataMat = rbind( dataMat, cbind( yVec, rep(sIdx, idealNtrlPerSubj) ) )
}
```

## `f`
```{r, eval=TRUE, cache=TRUE}
# Convert to a data frame:
idealDatFrm = data.frame(dataMat)

# Run Bayesian analysis on idealized data:
## Trying to create a set of representative parameter values for power analysis
## Want each successive step of joint parameter values to be clearly distinct,
## I.e., we want chains with very small autocorrelation. So thin the chains
## I.e., only generate as many steps in the chain as need by power analysis
## **we now have a large set of representative parameter values for conducting a power analysis

# Actual output from running baysiean analysis: 
## creates a posterior distribution on parameters ω and κ (as well as all the individual θs)

mcmcCoda = genMCMC(data=idealDatFrm, 
                   saveName=NULL, 
                   numSavedSteps=2000, 
                   thinSteps=20 
                   )

# Convert coda object to matrix for handling:
mcmcMat = as.matrix(mcmcCoda)
```

## `g`

### Changes to `Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power.R`
```{r, eval=FALSE}
# Changes to `Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power.R`...

nSimulatedDataSets = 50 # min(500,NROW(mcmcMat)) 
```

### Run the Power Model
```{r, eval=TRUE, cache=TRUE}
source('Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power.R')
```

### Why do the results differ from 13.2.5 of JK's book?
* The sample is similar to the book
* However, the results vary since we used 1 simulations instead of 500. 
* The MCMC data differs due to random data generation as well.

## `h` Changes to `Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power-1.R`
```{r 5h1, eval = FALSE}
# Changed in 'Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power-1.R'...
# Assumes changes from last part

# Specify idealized hypothesis:
idealGroupMean   = 0.44 # Matches the actual data group mean
idealGroupSD     = 0.04 # Matches the actual data SD
idealNsubj       =   28 # Matches the actual data number of subjects
idealNtrlPerSubj =   10 # Matches the actual data trials per subject
```

```{r 5h2, eval = TRUE, cache=TRUE}
# Note the -1 at end. 
source('Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power-1.R')
```

## `i` Changes to `Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power-2.R`
```{r 5i1, eval = FALSE}
# Changed in 'Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power-2.R'...
# Assumes changes from last part

# Specify criteria for goals:
nullROPE = c(0.48,0.52)
HDImaxWid = 0.1
 
# Specify sample size for each simulated data set:
Nsubj = 40 ; NtrlPerSubj = 100
#Nsubj = 2*7 ; NtrlPerSubj = 47 # 658 flips total
nSimulatedDataSets = 20 # still 1 since takes too long to run for an exercise
```

```{r 5i2, eval = TRUE, cache=TRUE}
# Note the -2 at end of file name
source('Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power-2.R')
```

### Question:
* `omegaNarrowHDI` has high power because there are 40 subjects.
* This allows for a constent group-level estimate of omega that is decently certain (HDI width less than
0.1)
* However, `thetasNarrowHDI` has low power since 100 trials per subject usually yields HDIs on individual theta’s such that at least one of them has width greater than 0.1.


## `j`

### Changes to `Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power-3.R`
```{r, eval=FALSE}
# Changed in 'Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power-3.R'...
# Assumes changes from last part

idealDatFrm = data.frame(dataMat)
idealDatFrm = read.csv("TherapeuticTouchData.csv") # overwrites previous idealDatFrm
# Run Bayesian analysis on idealized data:
mcmcCoda = genMCMC( data=idealDatFrm , saveName=NULL ,
 numSavedSteps=2000 , thinSteps=20 )
```

### Run the model
```{r 5j, cache=TRUE}
# Note the -3 at end of file name
source('Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power-3.R')
```

### Question
* Power estimates  similar last question: high power for `omegaNarrowHDI` and low power for `thetasNarrowHDI`

---

<br>

# Task 6

## `a` Proposal Function `myprop()`
```{r myprop}

```


## `b` Call `myprop()`
```{r mypropCall}

```


## `c` Barplot of `myprop()`
```{r theBar}

```


## `d` Liklihood Latex 


## `e` ℎ(𝜃) Beta prior


## `f` Acceptance Equation


## `g` Create function `mymcmc()`
```{r mymcmc}

```


## `h` Call function `mymcmc()`
```{r mymcmcCall}

```

