---
title:    "Assignment 3"
subtitle: "Bayesian Statistics"
author:   "Daniel Carpenter"
date:     "April 2022"
fontsize: 12pt
geometry: margin=1in
output:
  html_document:
    toc: yes
    toc_float: yes
  # github_document:
  #   toc: yes
  #   # number_sections: yes
  #   toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message=FALSE,
	warning=FALSE
)
```


All questions and parts done completely. Note Task 5 part g only ran one simulation but please not the remaining run entirely.   

---

# Task 1

## `a`-`b`

```{r}
# The data input
y = c( rep(1,9),rep(0,3), rep(1,45),rep(0,15), rep(1,3),rep(0,9) ) 
s = c( rep("A",12), rep("B",60), rep("C",12) )
fileName = "Ass3.1.csv"
write.csv( data.frame(y=y,s=s), file=fileName, row.names=FALSE )


# Example for Jags-Ydich-XnomSsubj-Mbernbeta.R 
#------------------------------------------------------------------------------- 
# Optional generic preliminaries:
graphics.off() # This closes all of R's graphics windows.

#------------------------------------------------------------------------------- 
# Load The data 
myData = read.csv(fileName)

# Include head of data
head(myData)

# N.B.: The functions below expect the data to be a data frame, 
# with one component named y being a vector of integer 0,1 values,
# and one component named s being a factor of subject identifiers.
myData$s = factor( myData$s )

#------------------------------------------------------------------------------- 
# Load the relevant model into R's working memory:
source("Jags-Ydich-XnomSsubj-MbernBeta.R")

#------------------------------------------------------------------------------- 
# Optional: Specify filename root and graphical format for saving output.
# Otherwise specify as NULL or leave saveName and saveType arguments 
# out of function calls.
dir.create('Output')
fileNameRoot = "Output//"
graphFileType = "png" 

#------------------------------------------------------------------------------- 
# Generate the MCMC chain:
mcmcCoda = genMCMC( data=myData, numSavedSteps=50000, saveName=fileNameRoot )

#------------------------------------------------------------------------------- 
# Display diagnostics of chain, for specified parameters:
parameterNames = varnames(mcmcCoda) # get all parameter names
for ( parName in parameterNames ) {
  diagMCMC( codaObject=mcmcCoda, parName=parName, 
            saveName=fileNameRoot, saveType=graphFileType )
}

# Ensure that the graphs of theta[1] and [2] are shown
knitr::include_graphics(paste0(fileNameRoot, 'Diagtheta[1].', graphFileType), dpi = 5)
knitr::include_graphics(paste0(fileNameRoot, 'Diagtheta[2].', graphFileType), dpi = 5)

#------------------------------------------------------------------------------- 
# Get summary statistics of chain:
summaryInfo = smryMCMC( mcmcCoda, compVal=NULL, rope=c(0.45,0.55),
                        compValDiff=0.0, ropeDiff = c(-0.05,0.05),
                        saveName=fileNameRoot )

# Display posterior information:
plotMCMC( mcmcCoda, data=myData, compVal=NULL, rope=c(0.45,0.55),
          compValDiff=0.0, ropeDiff = c(-0.05,0.05),
          saveName=fileNameRoot, saveType=graphFileType )
```


## `c` Credibility of Model
* The MCMC estimation appears credible across all values of theta since:
  * Param. Values seem to be around the mean value, which is stationary
  * Shrink factor evens out
  * Autocorrelation drops quickly
  * Each chain's desnity represents each other well when viewing superimposed over each other

## `d` Sample Size Effect
* Increased sample size of subject B will cause for a tighter distribution on the estimation

```{r}
# Add the histogram
plotPost(mcmcCoda[,'theta[1]'], xlim = c(0,1), xlab= expression(theta[1]),
         main = 'Daniel Carpenter', col = 'darkseagreen3',
                      showCurve=FALSE)

# Add the density line
paramSampleVec = as.matrix(mcmcCoda[,'theta[1]'])
densCurve = density( paramSampleVec, adjust=2 )
a = densCurve$x; b = densCurve$y

lines(a, b, type="l", lwd=2.5, col='darkseagreen4', bty="n")


graphics.off() # This closes all of R's graphics windows.
```

## `e`-`f` Lines()
* `a` and `b` resemble come from the `density()` function from the `mcmcCoda[,'theta[1]']` object
* The lines command adds a line to a plot as a layer.

## `h` Functions
i. `xlab= expression(theta[1])`
ii. `col = 'green3'`; note I used `col = 'darkseagreen3'`
iii. `main = 'Daniel Carpenter'`

## `i` Lines()
* You can use `showCurve = TRUE` but it does not plot both the histogram and the line, so I manually added the line.



---

<br>

# Task 2 Shrinkage

## `a` Latex Explanation
* Understand shrinkage (consquence to heirarchical model).
* Use the MLE approach
* Goal is to find the values of the parameters in the formula that will maximize the probability of the data.

$$
\begin{aligned}
&p\left(\left\{y_{i \mid s}\right\} \mid\left\{\theta_{s}\right\}, \omega, \kappa\right) \\
&=\prod_{s} \prod_{i \mid s} p\left(y_{i \mid s} \mid \theta_{s}, \omega, \kappa\right) \\
&=\prod_{s} \prod_{i \mid s} \operatorname{bern}\left(y_{i \mid s} \mid \theta_{s}\right) \cdot \operatorname{beta}\left(\theta_{s} \mid \omega(\kappa-2)+1,(1-\omega)(\kappa-2)+1\right)
\end{aligned}
$$

## `b` Proportions of Heads
```{r propHead}
y_s1 = c(1,0,0,0); paste('Proportion of Heads for y[s[1]]:', sum(y_s1) / sum(y_s1>=0)) # for y_s1
y_s2 = c(1,1,0,0); paste('Proportion of Heads for y[s[2]]:', sum(y_s2) / sum(y_s2>=0)) # for y_s2
y_s3 = c(1,1,0,0); paste('Proportion of Heads for y[s[3]]:', sum(y_s3) / sum(y_s3>=0)) # for y_s3
y_s4 = c(1,1,1,0); paste('Proportion of Heads for y[s[4]]:', sum(y_s4) / sum(y_s4>=0)) # for y_s4
```

## `c-e` Shrinkage Param. Set 1
```{r likFun}
# Function to estimate the liklihood from the MLE
likFromMLE <- function(N, z, omega, kappa, theta) {
  alpha = omega * (kappa-2) + 1 
  beta = (1-omega) * (kappa-2) + 1
  mleProb = theta^z * (1-theta)^(N-z) * dbeta(theta, alpha, beta)
  lik = prod(mleProb) 
  
  # Create a plot
  plot(x=seq(1/length(theta),1,1/length(theta)), y=dbeta(theta, alpha, beta), 
       main = 'Beta in Likelihood Function - Daniel Carpenter', 
       xlab=expression(theta),
       type = "b", pch = 19, col = 'steelblue3')
  
  paste('Likelihood Estimate:', lik)
  return(lik) # Return the likelihood
}
```
> Likelihood when ðœ”=0.5, ðœ…=2, ðœƒ1=0.25, ðœƒ2=0.50, ðœƒ3=0.50, and ðœƒ4=0.75?  

* `c` See output below  
* `d` These parameters do not since it is uniform  
* `e` The shape is flat (see output below)  

```{r 2c_e}
# Inputs 
N     = rep(4,4) 
omega = 0.5 
kappa = 2 
theta = c(0.25, 0.50, 0.50, 0.75) 
z = sum(y_s1, y_s2, y_s3, y_s4) # Sum of the heads

# Call the function
likFromMLE(N, z, omega, kappa, theta)
```

## `f`-`i` Shrinkage Param. Set 2
> Likelihood when ðœ”=0.5, ðœ…=2, ðœƒ1=0.35, ðœƒ2=0.50, ðœƒ3=0.50, and ðœƒ4=0.65?  

* `c` See output below  
* `d` These parameters do increase the likelihood (see the summary distribution below compared to the previous problem)
* `e` The shape is curved (see output below)  

```{r 2f_i}
# Inputs 
N     = rep(4,4) 
omega = 0.5 
kappa = 20 
theta = c(0.35, 0.50, 0.50, 0.65) 
z = sum(y_s1, y_s2, y_s3, y_s4) # Sum of the heads

# Call the function
likFromMLE(N, z, omega, kappa, theta)
```

## `j` Shrinkage affect on Liklihood
> What does shrinkage do to likelihood?

* Shrinkage occurs because the estimate of each low-level parameter is influenced from two
sources:
1. the subset of data that are directly dependent on the low-level parameter, and
2. the higher-level parameters on which the low-level parameter depends.

* Shrinkage improves the likelihood estimate drastically if the parameters in the formula do so.


---

<br>

# Task 3 Fair Coins

## `a-b` Two-tailed p value and Code Expanation?  
Generally, the two-tailed p-value is simply two times the one-tailed `p` value. See one-tailed description in part `c`

### Fix `N`:  
$$
p(z \mid N, \theta)=
\left(\begin{array}{c}  N \\ z  \end{array}\right) 
\theta^{z}(1-\theta)^{N-z}
$$


```{r 1a}
N = 45 ; z = 3 ; theta = 1/6  # Data as defined in problem 3 description

lowTailZ = 0:z # Consider the low tail because z/N = 3/45 is less than expected p=1/6:

# Calculates the cumulative lower tail probability of getting z success out of N trials is
# Multiply by two to get the two-tail p value
paste('Two tailed p-value:', 2 * sum( choose(N,lowTailZ) * theta^lowTailZ * (1-theta)^(N-lowTailZ) ) )
```


## `c` Why does it consider the low tail and not the high tail?
* Since $(z / N)_{\text {actual }} < E\left[(z / N)_{\theta, I}\right]$, we use lower tail. 
* If $(z / N)_{\text {actual }} > E\left[(z / N)_{\theta, I}\right]$, then we would use the upper tail.
* E.g., $(z/N = 3/45) \leq (p=1/6)$


## `d` Explain the meaning of the final result.
* The final results gives the probability `p` of getting `z` heads out of `N` flips
* Two-tailed p-value is $\geq 0.05$, so we should not reject the hypothesis that $\theta = 1/6$

## `e`-`f`

### Explain the code
```{r 1e}
sum( (lowTailZ/N) * choose(N,lowTailZ) * theta^lowTailZ * (1-theta)^(N-lowTailZ) )


# The cumulative lower tail probability is a negative binomial, 
# however, the tail is over n >= N, so we compute as  1-p(n<N)
complN = z:(N-1)
complP = sum( (z/complN) * choose(complN,z) * theta^z * (1-theta)^(complN-z) )
lowTailP = 1-complP

# Two-tail probability:
TwoTailP = 2 * lowTailP
show( TwoTailP )
```

### Explain the results
* two-tailed p value is $< 0.05$, so reject the hypothesis that $\theta = 1/6$. 
* Difference in part A due to different stopping intention when establishing the sampling distribution.

---

<br>


# Task 4 Dicotomous Outcome
> Continuing from `Task 3` with a dichotomous outcome  with `N` = 45 and `z` = 3. 

## `a` Stop when `N` = 45. What is the 95% CI?
```{r 4a}
N = 45 # Number of Trials
z = 3  # Number of Successes

# Low Tail - iterate over values of theta from 0.170 to 0.190 in increments of 0.001
for ( theta in seq( 0.170, 0.190, 0.001) ) {
  
  show( # Show the list object for each value of theta
    c( # Create a list including value of theta and the 2-tail probability
      
      # Get the two-tail probability using the LOWER tail estimate. Multuply by 2 for 2 tail
      # Interpret as the probability p of getting z heads for a given value of theta
      theta, 2*sum( choose(N,lowTailZ) * theta^lowTailZ * (1-theta)^(N-lowTailZ))
    )
  ) 
}

highTailZ = z:N 

# High Tail - iterate over values of theta from 0.005 to 0.020 in increments of 0.001
for ( theta in seq( 0.005, 0.020, 0.001) ) { 
  
  show( # Show the list object for each value of theta
    c( # Create a list including value of theta and the 2-tail probability
      
      # Get the two-tail probability using the UPPER tail estimate. Multuply by 2 for 2 tail
      # Interpret as the probability p of getting z heads for a given value of theta
      theta, 2*sum( choose(N,highTailZ) * theta^highTailZ * (1-theta)^(N-highTailZ) ) 
    )
  ) 
}
```

* We can be 95% confident that values of theta will fall between $0.014$ and $0.182$

## `b` Explain above code
* See comments above for details and interpretation
* 
## `c` Stop when `z` = 3, what is the 95% CI?
* Note altered the code by adding `(lowTailZ/N) *` and `(highTailZ/N) *`
```{r 4c}
# For candidate theta values GREATER than z/N observed, compute the LEFT-tail p-value:
complN = z:(N-1)

# Low Tail - iterate over values of theta
for ( theta in seq( 0.150, 0.160, 0.001) ) {
  
  show( # Show the list object for each value of theta
    c( # Create a list including value of theta and the 2-tail probability
      
      # Get the two-tail probability using the LOWER tail estimate. Multuply by 2 for 2 tail
      # Interpret as the probability p of getting z heads for a given value of theta
      theta, 2*(1-sum( (z/complN) * choose(complN,z) * theta^z * (1-theta)^(complN-z) ) )

    )
  ) 
}

# For candidate theta values LESS than z/N observed, compute the RIGHT-tail p-value:
highTailN = z:N # Notice N not N-1

# High Tail - iterate over values of theta from 0.005 to 0.020 in increments of 0.001
for ( theta in seq( 0.005, 0.020, 0.001) ) { 
  
  show( # Show the list object for each value of theta
    c( # Create a list including value of theta and the 2-tail probability
      
      # Get the two-tail probability using the UPPER tail estimate. Multuply by 2 for 2 tail
      # Interpret as the probability p of getting z heads for a given value of theta
      theta, 2*sum((z/highTailN) * choose(highTailN,z) * theta^z * (1-theta)^(highTailN-z)) 
    )
  ) 
}
```

## `d` Is the CI the same as for stopping when N = 45? 
* When the stopping at fixed z, the confidence interval is from $0.014$ to $0.154$

---

<br>

# Task 5

## `a` Explain Graph

* Overview: start with the actual or idealized data and then use Bayesâ€™ rule to generate the corresponding distribution on parameter values

1. Real or hypothetical world creates an actual or an idealized data set
2. Bayesâ€™ rule is the applied, which creates a posterior distribution
3. Posterior distribution is used as the hypothetical distribution of parameter values for power analysis

**Interpret code:**  

* Some `#` comments copied and pasted, ***but added own comments to ensure intepretation is right***

## `b`
```{r, eval=TRUE, cache=TRUE}
# Get key functions: genMCMC, smryMCMC, and plotMCMC, and get DBDA2E-utilities.R
source("Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa.R")

# The idealized hypothesis. This could be anecdotal evidence
idealGroupMean = 0.65
idealGroupSD   = 0.07

# Specify how much (idealized) data we have to support that hypothesis. 
# The more data, the more confidence in the hypothesis
idealNsubj       = 100  # more subjects => higher confidence in hypothesis
idealNtrlPerSubj = 100  # more trials => higher confidence in hypothesis
```

## `c`
```{r, eval=TRUE, cache=TRUE}
# goal is to generate data that supports the claims above:

# Generate random theta values for idealized subjects:
betaAB = betaABfromMeanSD( idealGroupMean, idealGroupSD )
theta = rbeta( idealNsubj, betaAB$a, betaAB$b )
```

## `d`
```{r, eval=TRUE, cache=TRUE}
# Transform the theta values to exactly match idealized mean, SD:
theta = ((theta-mean(theta))/sd(theta))*idealGroupSD + idealGroupMean
theta[ theta >= 0.999 ] = 0.999 # must be between 0 and 1
theta[ theta <= 0.001 ] = 0.001 # must be between 0 and 1

# Generate idealized data very close to thetaâ€™s:
z = round( theta*idealNtrlPerSubj )
```

## `e`
```{r, eval=TRUE, cache=TRUE}
# Convert to data format needed by JAGS function:
# Set up an empty matrix for holding the data:
dataMat=matrix(0,ncol=2,nrow=0,dimnames=list(NULL,c("y","s")))

# For each simulated subject,
for ( sIdx in 1:idealNsubj ) {
  
  # Create vector of 0â€™s and 1â€™s matching the z values generated above:
  yVec = c(rep(1,z[sIdx]),rep(0,idealNtrlPerSubj-z[sIdx]))
  
  # Bind the subject data to the bottom of the matrix:
  dataMat = rbind( dataMat, cbind( yVec, rep(sIdx, idealNtrlPerSubj) ) )
}
```

## `f`
```{r, eval=TRUE, cache=TRUE}
# Convert to a data frame:
idealDatFrm = data.frame(dataMat)

# Run Bayesian analysis on idealized data:
## Trying to create a set of representative parameter values for power analysis
## Want each successive step of joint parameter values to be clearly distinct,
## I.e., we want chains with very small autocorrelation. So thin the chains
## I.e., only generate as many steps in the chain as need by power analysis
## **we now have a large set of representative parameter values for conducting a power analysis

# Actual output from running baysiean analysis: 
## creates a posterior distribution on parameters Ï‰ and Îº (as well as all the individual Î¸s)

mcmcCoda = genMCMC(data=idealDatFrm, 
                   saveName=NULL, 
                   numSavedSteps=2000, 
                   thinSteps=20 
                   )

# Convert coda object to matrix for handling:
mcmcMat = as.matrix(mcmcCoda)
```

## `g`

### Changes to `Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power.R`
```{r, eval=FALSE}
# Changes to `Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power.R`...

nSimulatedDataSets = 50 # min(500,NROW(mcmcMat)) 
```

### Run the Power Model
```{r, eval=TRUE, cache=TRUE}
source('Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power.R')
```

### Why do the results differ from 13.2.5 of JK's book?
* The sample is similar to the book
* However, the results vary since we used 1 simulations instead of 500. 
* The MCMC data differs due to random data generation as well.

## `h` Changes to `Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power-1.R`
```{r 5h1, eval = FALSE}
# Changed in 'Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power-1.R'...
# Assumes changes from last part

# Specify idealized hypothesis:
idealGroupMean   = 0.44 # Matches the actual data group mean
idealGroupSD     = 0.04 # Matches the actual data SD
idealNsubj       =   28 # Matches the actual data number of subjects
idealNtrlPerSubj =   10 # Matches the actual data trials per subject
```

```{r 5h2, eval = TRUE, cache=TRUE}
# Note the -1 at end. 
source('Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power-1.R')
```

## `i` Changes to `Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power-2.R`
```{r 5i1, eval = FALSE}
# Changed in 'Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power-2.R'...
# Assumes changes from last part

# Specify criteria for goals:
nullROPE = c(0.48,0.52)
HDImaxWid = 0.1
 
# Specify sample size for each simulated data set:
Nsubj = 40 ; NtrlPerSubj = 100
#Nsubj = 2*7 ; NtrlPerSubj = 47 # 658 flips total
nSimulatedDataSets = 20 # still 1 since takes too long to run for an exercise
```

```{r 5i2, eval = TRUE, cache=TRUE}
# Note the -2 at end of file name
source('Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power-2.R')
```

### Question:
* `omegaNarrowHDI` has high power because there are 40 subjects.
* This allows for a constent group-level estimate of omega that is decently certain (HDI width less than
0.1)
* However, `thetasNarrowHDI` has low power since 100 trials per subject usually yields HDIs on individual thetaâ€™s such that at least one of them has width greater than 0.1.


## `j`

### Changes to `Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power-3.R`
```{r, eval=FALSE}
# Changed in 'Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power-3.R'...
# Assumes changes from last part

idealDatFrm = data.frame(dataMat)
idealDatFrm = read.csv("TherapeuticTouchData.csv") # overwrites previous idealDatFrm
# Run Bayesian analysis on idealized data:
mcmcCoda = genMCMC( data=idealDatFrm , saveName=NULL ,
 numSavedSteps=2000 , thinSteps=20 )
```

### Run the model
```{r 5j, cache=TRUE}
# Note the -3 at end of file name
source('Jags-Ydich-XnomSsubj-MbinomBetaOmegaKappa-Power-3.R')
```

### Question
* Power estimates  similar last question: high power for `omegaNarrowHDI` and low power for `thetasNarrowHDI`

---

<br>

# Task 6

## `a` Proposal Function `myprop()`
```{r myprop}
# Function to sample n tosses with head proposed has a probability p
# proposed 2-values in vector. First value is success, second is failure
myprop <- function(n, p, proposed=c(0.3, 0.6)){ 
  proposalDist <- sample(proposed, size = n,
                         replace = TRUE, prob = c(p, 1-p)) 
  
  return(list('proposalDist' = proposalDist, 'n'=n, 'p'=p))
}
```


## `b` Call `myprop()`
```{r mypropCall}
# Proposed c(0.3, 0.6) by default...
myprop(n=10,  p=0.5) #  10 tosses with probability of success being 0.5
myprop(n=20,  p=0.1) #  20 tosses with probability of success being 0.1
myprop(n=100, p=0.4) # 100 tosses with probability of success being 0.4
```


## `c` Barplot of `myprop()`
```{r theBar}
# Create a bar plot given the output of a myprop function
createBarPlot <- function(myprop) {

  # Colors to reuse  
  fillColor   = c('darkseagreen', 'skyblue')
  borderColor = c('darkseagreen4', 'skyblue4')

  # Create the barplot
  barplot(table(myprop$proposalDist) / myprop$n, 
          main = paste0('Proposal Distribution from myprop(n,p) | p = ',myprop$p,'\n Daniel Carpenter'),
          col  = fillColor, border = borderColor,
          xlab = expression(theta), ylab = 'Probability')
  
  # Create the legend
  legend('topleft', legend=c("Outcome of 0.3", "Outcome of 0.6"), 
         bty = 1, fill=fillColor, border=borderColor, box.col = 'grey90')
}

n = 1000 # number of tosses
p = 0.3  # Probability of a success, in this case is outcome of 0.3

# Call the bar plot function
# Note proposed c(0.3, 0.6) by default...
theProposal <- myprop(n, p)
createBarPlot(theProposal)
```

<br>

## `d` Binomial Liklihood Formula $p(x \mid \theta)$
$$ p(x \mid \theta) = \left(\begin{array}{l}n \\ x\end{array}\right) \theta^{x}(1-\theta)^{n-x} $$

<br>

## `e` Use Beta Prior $p(\theta)$ and $p(x \mid \theta)$ to get $h(\theta)$ 

### Beta distribution definition p(\theta)  
$$ p(\theta) = \frac{1}{B(\alpha, \beta)} \theta^{\alpha-1}(1-\theta)^{\beta-1}$$

### $h(\theta)$: Beta Prior $\times$ Binomial Likelihood  

> Note the parameter values are $\alpha = \beta = 2$ in this example  

$$ h(\theta) = p(\theta) p(x \mid \theta) $$

### $h(\theta)$ Explicitly stated  
$$ 
h(\theta) = 
\frac{1}{B(\alpha, \beta)} \theta^{\alpha-1}(1-\theta)^{\beta-1}
\times
\left(\begin{array}{l}n \\ x\end{array}\right) \theta^{x}(1-\theta)^{n-x}
$$

<br>

## `f` Acceptance Equation using $h(\theta)$
$$ \alpha_{i, j}=\min \left\{1, \frac{h_{j}}{h_{i}}\right\} $$

<br>

## `g` Create function `mymcmc()`
```{r mymcmc}
mymcmc <- function(n=10000, h, p=0.5, proposed, ...){

  # Function to sample n tosses with head proposed has a probability p
  # proposed 2-values in vector. First value is success, second is failure
  myprop <- function(n, p, proposed){ 
    proposalDist <- sample(proposed, size = n,
                           replace = TRUE, prob = c(p, 1-p)) 
    
    return(list('proposalDist' = proposalDist, 'n'=n, 'p'=p))
  }
  alpha<-c() # holds transition probs
  alpha[1]<-1
  u<-c() # holds uniform values
  u[1]<-1
  post<-c()# post sample
  prop<-c() # vec of proposed states 1s and 2s
  prop[1]=proposed[1] # initial state
  post[1]=prop[1]
  
  for(i in 2:n){ # starts at 2 because initial value given above
    # proposal state 
    prop[i]=myprop(n=1, p=p, proposed=proposed)$proposalDist   
    
    # calculate alpha
    # notice h[prop[i]] gives the actual value of h
    alpha[i]=min(1,h[prop[i]]/h[post[i-1]])
    
    # to calculate accepting proposal with prob alpha
    # select a random value from  a uniform (0,1)
    u[i]=runif(1)
    if(u[i]<=alpha[i]){post[i]<-prop[i]}
    else{post[i]<-post[i-1]}
  }
  res<-matrix(c(prop,u,alpha,post ),nc=4,nr=n,byrow=FALSE)
  sim<-table(post)/n
  # windows only works with a pc
  # quartz with macs
  # dev.new(noRStudioGD = TRUE) # or quartz() 
  
  postexact<-h/sum(h)
  
  
  # PLOTS ======================================================================
  
  require(tidyverse)
  require(ggplot2)
  
  # Create a simple theme for reuse
  myTheme <- theme_minimal() + theme(text = element_text(color = '#666666'),
                                     panel.grid.major = element_blank())
  
  
  # Make the iter vector a data frame
  iterOut <- as.data.frame(res) %>% 
    mutate(numIterations = row_number()) 
  
  
  
  # Show Proposal  -------------------------------------------------------------
  propPlot <- ggplot(data.frame(table(prop)/n),
                     aes(x=prop, y = Freq)) +

    # COlumn Chart of the post estimated
    geom_col( fill="lightblue", colour="lightblue4", alpha = 0.5) +

    # Theme
    myTheme +

    # Labels
    labs(title = 'Proposal', y = "Probability", x = expression(theta) )
  

  # Posterior simulation ------------------------------------------------------
  postPlot <- ggplot(data.frame(table(post)/n), 
                     aes(x=post, y = Freq)) + 
  
    # COlumn Chart of the post estimated
    geom_col( fill="tomato3", colour="tomato4", alpha = 0.5) +
    
    # Theme
    myTheme + 
    
    # Labels
    labs(title = "Posterior", y = "Probability", x = expression(theta) )
  
  
  # Posterior Trace Plot  ------------------------------------------------------
  tracePostPlot <- ggplot(data=iterOut,
                          aes(x=numIterations, y=V4)) +
    
    # Add the lines
    geom_line(color='grey75', alpha = 0.7) +
    
    # Add some points
    geom_point(color = 'grey40', size = 0.25, alpha = 0.1) + 
    
    # Theme and Labels
    myTheme +
    labs(title = "Trace Plot", x = "Iterations", y = expression(theta) )
  
  
  # Print the plots on single grid ---------------------------------------------
  require(cowplot) # allows for plotting on single grid using plot_grid())
  print(plot_grid(propPlot, postPlot, tracePostPlot,
            nrow = 1, rel_widths = c(1,1,2)))

  
  # The returned output is a list 
  # Use obj$ to obtain whatever interests you
  return(list(iter=res,sim=sim,postexact=postexact,post=post) )
}
```


## `h` Call function `mymcmc()`
```{r mymcmcCall}

# Create a function to get a bernnouli lik and beta prior
getBernBetaH <- function(numThetaValues, 
                         x, n, 
                         shape1=2, shape2=2) {
  
  ## Form uniform probability
  theta <- seq(0, 1, length = numThetaValues)
  
  ## Calculate prior assuming uniform distribution
  prior = dbeta(theta, shape1, shape2)
  
  ## Calculate the likelihood
  likelihood  = dbinom(x=x, size=n, prob=theta)
  
  ## Calculate the Prior x the Likelihood
  h <-  prior * likelihood
  
  return(h)
}


# Inputs for getBernBetaH()
numThetaValues = 1000

# Proposal Inputs 
p=0.5
proposed=c(0.3,0.6)

# Beta prior inputs
shape1 = 2
shape2 = 2

# Bernoulli Likelihood inputs
x = 4
n = 10

# h: prior x likelihood using beta prior and bernoulli likelihood
h=getBernBetaH(numThetaValues, x, n, shape1,shape2)

# Call mydmcmc and add some extras to the plot
obj <- mymcmc(n=50000, h, p=0.5, proposed=c(0.3,0.6))
# head(obj) # not required since prints already
```

