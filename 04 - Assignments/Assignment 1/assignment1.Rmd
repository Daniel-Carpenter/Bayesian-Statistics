---
title:    "Assignment 1"
subtitle: "Bayesian Statistics"
author:   "Daniel Carpenter"
date:     "February 2022"
fontsize: 12pt
geometry: margin=1in
output:
  html_document:
    toc: yes
    toc_float: yes
  pdf_document: 
    toc: TRUE
    number_sections: TRUE
  github_document: default
  md_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

---

# Task `1`: Bayes Box

## `a.` Classical Point Estimate for ùúÉ (*Probability of Success*)
```{r 1a, echo=TRUE, message=FALSE}
# Function that creates a classical point est
classicalPointEst <- function(n) {
  theta <- seq(0, 1, length = n)
  
  # Point Estimate for Theta
  return(mean(theta))
}

n = 10 # Num Trials
classicalPointEst(n)
```


## `b.` Find classical 95% confidence interval using $\theta$
```{r 1b, echo=TRUE, message=FALSE}
# Function for classical CI est at 95% confidence
classicalConfInt95 <- function(classicalPointEstFun = classicalPointEst, n) {
  # Get the classical point est
  pointEst <- classicalPointEstFun(n)
  
  # Return the classical interval
  return(pointEst + c(-1, 1) * 1.95 * sqrt(pointEst*(1 - pointEst) / n))
}
  
classicalConfInt95(classicalPointEst, n)
```


## `c.` `mycoin()`: Bayes Box and Related Data

### Create the function `mycoin()`
```{r 1ci, echo=TRUE, message=FALSE, warning=FALSE}
mycoin <- mybinpost <- function(n, x, p, prior, alpha) {
  
  # CALCULATIONS -------------------------------------------
    
    ## Get the length of p
    numRows <- length(p)
  
    ## Calculate the likelihood
    likelihood  = dbinom(x=x, size=n, prob=p)
    
    ## Calculate the Prior x the Likelihood
    h <-  prior * likelihood
    
    ## Get the posterior Distribution
    posterior = h / sum(h) 
    
    ## Consolidate into a matrix with row and column names
    bayesMatrix <- matrix(c(p, prior, likelihood, h, posterior), 
                        nr = numRows, nc = 5, byrow = FALSE)
    colnames(bayesMatrix) <- c("p", "prior", "likelihood", "h", "posterior")
    rownames(bayesMatrix)= c(rep("", numRows))
    rbind(bayesMatrix, colSums(bayesMatrix)) # Column totals

        
  # PLOTTING -----------------------------------------------
    
    ## Define some colors for the later plots
      red   = 'tomato3'
      blue  = 'steelblue'
      green = 'darkseagreen'
      colorPalette <- c(blue, green, red) # Consolidate in color palette
    
    ## Number of Theta numbers to plot
    thetaNumericValues = 1:numRows
    
    ## Convert to data frame for ggplot
    df <- as.data.frame(cbind(bayesMatrix, thetaNumericValues))
    
    
    if(!require(tidyverse)) install.packages(tidyverse)
    df <- df %>%
      
      ### Select to only the needed data
      select(-h) %>%
      
      ### Pivot y axis variables into single column for ggplot-ing
      pivot_longer(cols      = c("prior", "likelihood", "posterior"),
                   values_to = "values",
                   names_to  = "statNames")
    
    ## Plot it!
    basePlot <- ggplot(df,
                       aes(x = thetaNumericValues,
                           y = values,
                           color = statNames)) +
      ### Theme and colors
      theme_minimal() +
      scale_color_manual(values = colorPalette) +
      
      ### Labels
      labs(title = "Prior, Likelihood, Posterior over the Discrete Values of Theta",
           subtitle = paste0("Daniel Carpenter | x = ", x, ", n = ", n),
           x = 'Number of Theta Values', y = 'Probability')+ 
      
      ### Create the points
      geom_point()
    
    # Output Plot
    print(basePlot)
    
    
  # BCI & Point Est ------------------------------------------
    
    ## BCI
    cp = cumsum(posterior) # cumulative sum
    L = max(which(cp<alpha/2)) # this gives the max index where  cp < alpha/2
    U = min(which(cp > 1-alpha/2))
    BCI = df$p[c(L,U)] # close to the desired BCI
    
    ## Bayesian point estimate is the posterior mean
    bayesPointEst <- mean(posterior)
    
    
  # CLasical Point Est and Interval 95% CI ----------------
      classicalPointEstimate <- classicalPointEst(n)    
      cCI95 <- classicalConfInt95(classicalPointEst, n)

          
  # RETURN DATA ---------------------------------------------
      
    # Create a folder for the Output to stay organized
    outputFolder <- 'Output/'
    dir.create(outputFolder)
    
    # Create the folder for this task
    task1Folder  <- paste0(outputFolder, 'Task_01/')
    dir.create(task1Folder)
    print(paste0('Please find the Output Files located at ', task1Folder))
    
    ## File name for writing data to current wd
    nameOfFile <- paste("", sum(p),sum(prior),n,x,alpha, sep = "_") # Used this name since unique identifier
    
    ## Write a CSV  
    write.csv(x = as.data.frame(bayesMatrix), 
              file = paste0(task1Folder, "BayesBox", nameOfFile, ".csv"))
      
    ## Write above plot to jpg
    ggsave(filename = paste0(task1Folder, "BayesPlot", nameOfFile, ".jpg"),
           plot     = basePlot,
           height   = 8.5,
           width    = 11)
    
    ## Used for latex output of a matrix
    if(!require(xtable)) install.packages(xtable)
    
    ## Return a list of relevant data
    return(list('bayesMatrix'      = bayesMatrix,
                'bayesPointEst'    = bayesPointEst,
                'bayesCredIntvl95' = BCI,
                'classicalPEst'    = classicalPointEstimate,
                'classicalCI95'    = cCI95))
}

```

### Call the function `mycoin()` with 3 sets of inputs
```{r 1cii, echo=TRUE, message=FALSE, warning=FALSE}
mycoin(p = seq(0,1,length=20), prior =rep(1/20, 20), n=10, x=4, alpha = 0.05)
mycoin(p = seq(0,1,length=40), prior =rep(1/40, 40), n=10, x=4, alpha = 0.05)
mycoin(p = seq(0,1,length=20), prior =rep(1/20, 20), n=10, x=4, alpha = 0.1)

# Assume Prior equaling the length of the p
pr =rep(1/40, 40)
mycoin(p = seq(0,1,length=40), prior = pr, n =10, x=4, alpha=0.05)
```
---

<br>

# Task `2`: Variance
> *THIS SHOWS AN EXAMPLE, SEE PART `ii/iii` OF `TASK 3` FOR FINAL PROOF OF $V(X)$*
  
**Inputs**  
```{r 2inputs, echo=TRUE, message=FALSE}
n = 1000  # Intervals
p = 0.5   # Probability
```

**Variance using raw Definition** $V(X) = \frac{E(X - \mu)^2}{n}$  
```{r 2var1, echo=TRUE, message=FALSE}

varMethod1 <- function(n, p) {
  # Calculate the binomial distribution using parameters n, p, and q
  mu = n * p
  X  = pbinom(q=1-p, size=n, prob=p)
  
  # Calculate the variance of X using above formula 
  variance = (X - mu)^2 / n
  return(round(variance, 1))
}

varMethod1(n, p)
```

**Verify** $n \times p \times q$ **is the above variance**
```{r 2var2, echo=TRUE, message=FALSE}
varMethod2 <- function(n, p) {
  # Calculate q
  q = 1 - p
  
  # Calculate the variance
  variance2 <- n*p*q
  return(variance2)
}

varMethod2(n, p)
```


---

<br>

# Task `3`: MGF

## `i` 
Definition of The M.G.F of the Binomial Distribution:
$$
b(x ; n, p)=\frac{n !}{x !(n-x) !} p^{x} q^{n-x} \quad \text { with } \quad q=1-p .
$$

MGF Given by:
$$
\begin{aligned}
M(x, t) &=\sum_{x=0}^{n} e^{x t} \frac{n !}{x !(n-x) !} p^{x} q^{n-x} \\
&=\sum_{x=0}^{n} \frac{n !}{x !(n-x) !}\left(p e^{t}\right)^{x} q^{n-x} \\
&=\left(p e^{t}+q\right)^{n}
\end{aligned}
$$

Differentiate the MGF with respect to $t$ using the function-of-a-function rule:
$$
\begin{aligned}
\frac{d M(x, t)}{d t} &=n\left(q+p e^{t}\right)^{n-1} p e^{t} \\
&=n p e^{t}\left(p e^{t}+q\right)^{n-1}
\end{aligned}
$$

Now use $t=0$ to get $E(x)$
$$
E(x)=n p(p+q)^{n-1}=n p .
$$


## `ii. / iii.`
Find the second moment using product rule:
$$
\frac{d u v}{d x}=u \frac{d v}{d x}+v \frac{d u}{d x}
$$
$$
\begin{aligned}
\frac{d^{2} M(x, t)}{d t^{2}} &=n p e^{t}\left\{(n-1)\left(p e^{t}+q\right)^{n-2} p e^{t}\right\}+\left(p e^{t}+q\right)^{n-1}\left\{n p e^{t}\right\} \\
&=n p e^{t}\left(p e^{t}+q\right)^{n-2}\left\{(n-1) p e^{t}+\left(p e^{t}+q\right)\right\} \\
&=n p e^{t}\left(p e^{t}+q\right)^{n-2}\left\{q+n p e^{t}\right\} .
\end{aligned}
$$

Use $t=0$ again:
$$
\begin{aligned}
E\left(x^{2}\right) &=n p(p+q)^{n-2}(n p+q) \\
&=n p(n p+q)
\end{aligned}
$$

**Finally we now have derived $V(X)=E(X-\mu)^{2}$ (`i`) AND that the variance of $X$ is $n p q$ where $q=1-p$ (`ii.`).**
$$
\begin{aligned}
V(x) &=E\left(x^{2}\right)-\{E(x)\}^{2} \\
&=n p(n p+q)-n^{2} p^{2} \\
&=n p q
\end{aligned}
$$



---

<br>

# Task `4`: Normal Density

## Create Normal Density Function called `mynorm()`
```{r mynorm, echo=TRUE, message=FALSE}
    mynorm <- function(mu, sigma, 
                       lowerBound = NA, upperBound = NA, 
                       alpha,
                       roundTo = 4, 
                       color = paste0("lightsteelblue", floor(runif(1, min=1, max=4))),
                       returnCMD = TRUE)
    {
      
      # Calculate the "xlim" lower and upper bound for the Normal PDF Curve
      curveLowerBound <- mu - 3*sigma
      curveUpperBound <- mu + 3*sigma
      
      # Initialize variables related to output and graph
      title <- ""   # Title of graph
      exactProb = 0 # The exact probability of the questions
      
      # If no provided LOWER AND UPPER Bound (NA as parameter value) then assume none
      if (!(is.na(lowerBound)) & !(is.na(upperBound))) {
        title <- paste0(", P(", lowerBound, " <= X < ",upperBound,")")
        exactProb = pnorm(upperBound, mu, sigma) - pnorm(lowerBound, mu, sigma) # calculate prob 
        
      # If no provided LOWER Bound (NA as parameter value) then assume none
      } else if (is.na(lowerBound)) {
        lowerBound = curveLowerBound
        title <- paste0(", P(X < ",upperBound,")") # Set a dynamic title
        exactProb = pnorm(upperBound, mu, sigma) # calculate prob 
        
      # If no provided UPPER Bound (NA as parameter value) then assume none
      } else if(is.na(upperBound)) {
        upperBound = curveUpperBound
        title <- paste0(", P(X >= ",lowerBound,")")
        exactProb = 1 - pnorm(lowerBound, mu, sigma) # calculate prob 
      }
      
      # Create the line that displays the bell curve (between the CURVE bounds defined above)
      curve(
        
        ## Normally Distributed
        dnorm(x,mu,sigma), 
        
        ## Normally Distributed
        xlim=c(curveLowerBound, curveUpperBound), 
        
        ## Line width
        lwd =2, 
        
        ## Title with descriptive characteristics about function parameters
        main = paste0("Probability Distribution (by Daniel Carpenter)\n",
                      "X ~ N(",mu,", ",sigma,")", title),
        
        ## X and Y labels
        ylab = 'Probability Density',
        xlab = 'Value of X',
      )

      
      # Add the AREA of between the lower and upper bound P(lowerBound<X<=upperBound)
      
        ## X-Axis curve (length does not matter)
        xcurve = seq(lowerBound,upperBound, length=1000)
        
        ## Y-Axis Curve
        ycurve = dnorm(xcurve, mu,sigma)
        
        ## Combine the X and Y curve to form the area (in green)
        polygon(c(lowerBound, xcurve, upperBound), 
                c(0, ycurve, 0), 
                col=color) 
        
        ## Legend
        legend("topleft", legend="Area (of Prob. Density)", 
               fill=color, bty = "n")
      
      # Add the probability as text
        
        ## Calculate the area (probability)
        area = exactProb
        areaRounded = round(area, roundTo)
        
        ## Place this on the above plot
        text(12,0.02,substitute(paste("Probability = ", areaRounded), 
                                list(areaRounded = areaRounded)))
        
      # Quantiles
        
        ## Alpha (Lower Tail)
        lowerTail = qnorm(p = alpha / 2, 
                          mean = mu, sd = sigma,
                          lower.tail = TRUE)
        
        ## 1 minus alpha (upper tail)
        upperTail = qnorm(p = alpha / 2, 
                          mean = mu, sd = sigma,
                          lower.tail = FALSE)
        
      # Return stats about the Plot
      if (returnCMD) {
        return(list(shadedArea = areaRounded,
                    lowerTail = lowerTail,
                    upperTail = upperTail))
      }
    }
```

## `a.` Call Normal Density Function `mynorm()`
```{r 2a, echo=TRUE, message=FALSE}
mynorm(mu=10, sigma=8, 
       lowerBound = 8, upperBound = 11,
       alpha = 0.10)
```

---

<br>

# Task `5`: MLE

## `5.1.` Find  $\hat{\lambda}$ as a formula
* Do this by first finding the likelihood function which is shown as $L(\lambda)$
$$
L(\lambda)=\prod_{i=1}^{n} f_{X}\left(x_{i} ; \lambda\right)=\prod_{i=1}^{n}\left\{\frac{\lambda^{x_{i}}}{x_{i} !} e^{-\lambda}\right\}=\frac{\lambda^{x_{1}+\ldots+x_{n}}}{x_{1} ! \ldots x_{n} !} e^{-n \lambda}
$$

## `5.2.` Find the second derivative of $L(\lambda)$ as a formula.

### `5.2.1`: get $\log L(\lambda)$
$$
\log L(\lambda)=\sum_{i=1}^{n} x_{i} \log \lambda-n \lambda-\sum_{i=1}^{n} \log \left(x_{i} !\right)
$$

### `5.2.1`: Second derivative of $L(\lambda)$
$$
\frac{d}{d \lambda}\{\log L(\lambda)\}
=\sum_{i=1}^{n} \frac{x_{i}}{\lambda}-n=
0 \\ \text{so,}  \ \widehat{\lambda}=\frac{1}{n} 
\sum_{i=1}^{n} x_{i}=\bar{x}
$$


## `5.3/5.4.` Show $\hat{\lambda}$ is a maximum. (how will you do this?). Show using calculus that the max of ùêø(ùúÉ) is the same as the maximum of ùëô(ùúÉ)
* To check for the maximum, set $\lambda = \hat{\lambda}$ when in above derivation.
* Then check to see if negative

$$
\frac{d^{2}}{d \lambda^{2}}\{\log L(\lambda)\}
=-\frac{1}{\lambda^{2}} \sum_{i=1}^{n} x_{i}<0 \quad 
\text {where} \ \lambda=\widehat{\lambda}
$$

## `5.5.` Make a function called `myml(x)`
* Draws the graph of ùêø(ùúÜ) and ùëô(ùúÜ) with the x = vector of data
```{r fun5.5, echo=TRUE, message=FALSE}

myml = function(x) {
  
  # Create lambda vector as a set of values ranging from 0 to 2*max(x)
  lambda=seq(.01,2*max(x),0.5)
  
  # Calculate Poisson MLE
  lambdahat = round(sum(x)/length(x),4)
  
  #the likelihood function
  lik <- exp(-length(x)* lambda)*(lambda^sum(x))/prod(factorial(x))
  
  #the loglikelihood function
  loglik <- -length(x)* lambda+sum(x)*log(lambda)+log(prod(factorial(x)))
  # return(list(mle=lambdahat))
  
  # Plot the likelihood
  plot(lambda, lik,    
       col = 'steelblue', type = 'l', 
       main = 'Plot for Likelihood Lambda (Poisson MLE)\nDaniel Carpenter',
       xlab = 'Lambda', ylab = 'Likelihood')
  text(x=lambdahat, y=mean(lik), paste('L Hat=',round(lambdahat, 4)))
  
  # Plot the loglikelihood
  plot(lambda, loglik,
       col = 'tomato3', type = 'l', 
       main = 'Plot for Log(Likelihood) Lambda (Poisson MLE)\nDaniel Carpenter',
       xlab = 'Lambda', ylab = 'Log Likelihood')
  text(x=lambdahat, y=mean(loglik), paste('L Hat=',round(lambdahat, 4)))
  
  return(list(lambdahat = lambdahat))
}
```

> Give the output of your function when x = {3,4,3,5,6}.

```{r out5.5, echo=TRUE, message=FALSE}
x = c(3,4,3,5,6)
myml(x)
```
## `5.6.`
The maximum likelihood estimate does not account for prior information. We know the prior times the likelihood forms the posterior, which the M.L. Estimate only uses the likelihood.

---

<br>

# Task `6`: Un/Biased $\lambda$

Prove whether $\hat\lambda$ by proving $E(y) = \lambda$, for a random variable $Y$ ‚àº $Poisson(\lambda)$, $E(Y) = \lambda$

$$
\begin{aligned}
E(Y) &=\sum_{y=0}^{\infty} y \frac{e^{-\lambda} \lambda^{y}}{y !} \\
&=\sum_{y=1}^{\infty} y \frac{e^{-\lambda} \lambda^{y}}{y !} \\
&=\sum_{y=1}^{\infty} \frac{e^{-\lambda} \lambda^{y}}{(y-1) !} \\
&=\lambda e^{-\lambda} \sum_{y=1}^{\infty} \frac{\lambda^{y-1}}{(y-1) !} \\
&=\lambda e^{-\lambda} \sum_{y=0}^{\infty} \frac{\lambda^{y}}{y !} \\
&=\lambda e^{-\lambda} e^{\lambda} \\
&=\lambda
\end{aligned}
$$

Therefore, $E(Y) = \lambda$, which indicates that $\lambda$ is not biased.


---

<br>

# Task `7`: MLE for Bernoulli

Consider $n$ Bernoulli trials with $X$ successes.
$$
p(x)=\left(\begin{array}{l}
n \\
X
\end{array}\right) \theta^{X}(1-\theta)^{n-X}
$$

Compute the likelihood: $L(\theta)$
$$
L(\theta)=\left(\begin{array}{l}
n \\
X
\end{array}\right) \theta^{X}(1-\theta)^{n-X}
$$

Find the log(liklihood):
$$
\log L(\theta)=\log \left\{\left(\begin{array}{l}
n \\
x
\end{array}\right)\right\}+x \log (\theta)+(n-x) \log (1-\theta)
$$

Then, Find the derivative of log(likelihood)
$$
\frac{d}{d \theta} \log L(\theta)=\frac{x}{\theta}-\frac{n-x}{1-\theta}
$$

Set log(likelihood) to $0$
$$
\frac{x}{\theta}-\frac{n-x}{1-\theta}=0 \Longleftrightarrow x-\theta x=n \theta-\theta x \Longleftrightarrow \theta=\frac{x}{n}
$$

$\hat{\theta}=\frac{x}{n}$ is the candidate.  Now take the second derivative:
$$
\frac{d^{2}}{d \theta^{2}} \log L(\theta)=-\frac{x}{\theta^{2}}-\frac{n-x}{(1-\theta)^{2}}
$$

Above is  is always less than 0. Therefore $\hat\theta = \frac{X}{n}$ is the maximum likelihood estimator for $\theta$.


---

<br>

# Task `8`:

## `a.`
```{r 8a, echo=TRUE, message=FALSE}
```

## `b.`
```{r 8b, echo=TRUE, message=FALSE}
```

## `c.`
```{r 8c, echo=TRUE, message=FALSE}
```

## `d.`
```{r 8d, echo=TRUE, message=FALSE}
```

## `e.`
```{r 8e, echo=TRUE, message=FALSE}
```


---

<br>

# Task `9`: 

##  
```{r 9a, echo=TRUE, message=FALSE}
```


---

<br>

# Task `10`: 

## `a.` 
```{r 10a, echo=TRUE, message=FALSE}
```


## `b.` 
```{r 10b, echo=TRUE, message=FALSE}
```

