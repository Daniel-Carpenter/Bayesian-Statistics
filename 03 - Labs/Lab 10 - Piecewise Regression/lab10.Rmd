---
title: 'Lab 10: Piece-wise Linear Model'
subtitle: 'Bayesian Statistics'
author: "Daniel Carpenter"
date: "March, 2022"
output:
  # github_document:
  #   toc: yes
  #   # number_sections: yes
  #   toc_depth: 2
  html_document:
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Change Point Regression Models & Lab Overview

## Overview and Topics

* In this lab we will investigate another class of models which are extremely flexible in their application. 

* This topic can be seen as piecewise regression.

## The Change Point Model Expression

$$y_i \sim N(\mu_i, \sigma^2)$$
$$\mu_i = \beta_0 + \beta_1 x_i +  \beta_2(x_i -\theta)I_{(1, x_i\ge\theta,\; 0\; else)}$$


## Example: Stagnant Water: Change point model

$y_i$ is the log flow rate down an inclined channel, and $x_i$ is the  log height of stagnant surface layers of different surfactants. The rate of decline in flow rate seems to suddenly increase around $x=0$.

## Data Overview

```{r data}
dataList = list(
  y = c(1.12, 1.12, 0.99, 1.03, 
        0.92, 0.90, 0.81, 0.83, 
        0.65, 0.67, 0.60,  0.59, 
        0.51, 0.44, 0.43, 0.43, 
        0.33, 0.30, 0.25, 0.24, 
        0.13, -0.01, -0.13, -0.14, 
        -0.30, -0.33, -0.46,-0.43, -0.65),
  
  x = c(-1.39, -1.39, -1.08, -1.08, 
        -0.94, -0.80, -0.63, -0.63, 
        -0.25, -0.25, -0.12, -0.12, 
        0.01, 0.11, 0.11, 0.11,  
        0.25, 0.25, 0.34, 0.34, 
        0.44, 0.59, 0.70, 0.70, 
        0.85, 0.85,  0.99, 0.99, 1.19),
  
  N = 29)

```

# Task 1 - Review Data and 2-Linear Pieces

## Plot the data using ggplot

```{r plotdata, warning=FALSE}

df = data.frame(x = dataList$x, y=dataList$y)

head(df)

library(ggplot2)
basePlot <- ggplot(df, aes(x=x, y=y)) +
              geom_point() +
              geom_smooth(method = 'loess', se = TRUE) + 
              theme_minimal() +
              labs(title = 'Plot of Data',
                   subtitle = 'Daniel Carpenter')
basePlot
```

## Comment on the plot


# Task 2 - Create JAGS Model

## JAGS Model for Piece-wise Regression

```{r model, eval=FALSE}
model {
  for (i in 1:N) {
    y[i]    ~ dnorm(mu[i], tau)
    mu[i]  <- beta0 + beta[1]*x[i] + beta[2]*(x[i] - theta)
            * step(x[i] - theta)
  }
  tau       ~ dgamma(0.001, 0.001)
  beta0     ~ dnorm(0.0, 1.0E-6)
  
  for (j in 1:2) {
    beta[j] ~ dnorm(0.0, 1.0E-6)
  }
  sigma    <- 1/sqrt(tau)
  theta     ~ dunif(-1.3, 1.1)
  
  int2 <- beta0 - beta[1]*theta
  slope2 <- beta[1] + beta[2]
}
```

### `step()` Function 
* In the expression of the analytical model we used $I_{(1, x_i\ge\theta,\; 0\; else)}$

* What jags function is used for $I$? `step()`

### A prior is placed on $\tau$ 

What is the mean and variance of the prior distribution for $\tau$?
```{r}
# What it looks like
curve(dgamma(x, shape = 0.001, rate = 0.001), xlim = c(0, 10))
title(main = 'Daniel Carpenter')

# Mean
0.001/0.001

# Variance
0.001/0.001^2


# Variance
```



# Task 3 - Create Full-JAGS Scripts

## Create JAGS Script with Piece-wise Regression

Make a complete Jags script to run the model. You can use the script below and alter it to fit.

```{r jagscript, eval=TRUE, message=FALSE, warning=FALSE}
library(rjags)
#Define the model:
modelString = "
model {
  for (i in 1:N) {
    y[i]    ~ dnorm(mu[i], tau)
    mu[i]  <- beta0 + beta[1]*x[i] + beta[2]*(x[i] - theta)
            * step(x[i] - theta)
  }
  tau       ~ dgamma(0.001, 0.001)
  beta0     ~ dnorm(0.0, 1.0E-6)
  
  for (j in 1:2) {
    beta[j] ~ dnorm(0.0, 1.0E-6)
  }
  sigma    <- 1/sqrt(tau)
  theta     ~ dunif(-1.3, 1.1)
  
  int2 <- beta0 - beta[1]*theta
  slope2 <- beta[1] + beta[2]
}
" # close quote for modelString
writeLines( modelString , con="TEMPmodel.txt" )

initsList = list(tau = 1, beta0 = 2, beta = c(1, 2), theta = 1)

# Run the chains:
jagsModel = jags.model( file="TEMPmodel.txt" , data=dataList , inits=initsList , 
                        n.chains=3 , n.adapt=500 )

# Show how the model is ran - realSlicer is slow. Conjugate is fast
list.samplers(jagsModel)

update( jagsModel , n.iter=500 )
codaSamples = coda.samples( jagsModel , variable.names=c("tau", "sigma",
                                                         'beta', 'beta0', 'theta',
                                                         'int2', 'slope2'),
                            n.iter=33340 )
save( codaSamples , file=paste0("lab10","Mcmc.Rdata") )

# Output of model
summary(codaSamples)

# Plot the model output
library(ggmcmc)
s = ggs(codaSamples)
ggs_density(s)

ggs_crosscorrelation(s)
```


# Task 4 - Retrieve Point Estimates from JAGS Model

Run the model and make point and interval estimates.
```{r su}
su = summary(codaSamples)
su
```


### Interpret the estimates:  
* The mean and standard deviation for each monitored variable can be shown above. For example, the mean $\mu$ of $\beta_1$ is around -0.42 and its standard deviation $\sigma$ is around 1.499e-02  
* We can also analyze the credibility intervals for each variable. For example, there is a 95% probability that $\beta_1$'s mean will approximately fall between -0.449 and -0.389.   
* Above values rounded since each MCMC run varies in results (since it is an estimation)

# Task 5 - Plot Piece-Wise Estimates on Data

Plot the estimating lines onto the data
```{r task5Plot, echo=TRUE, message=FALSE, warning=FALSE}
# Create objects using summary stats
beta0  = su$statistics[,'Mean']['beta0']
beta1  = su$statistics[,'Mean']['beta[1]']
beta2  = su$statistics[,'Mean']['beta[2]']
int2   = su$statistics[,'Mean']['int2']
slope2 = su$statistics[,'Mean']['slope2']
theta  = su$statistics[,'Mean']['theta']

# Plot the two estimated linear pieces with the cuttoff point
basePlot + 
  geom_abline(intercept = beta0, slope = beta1)  + # Piece 1  
  geom_abline(intercept = int2,  slope = slope2) + # Piece 2
  geom_vline(xintercept = theta)                 + # Intersection 
  labs(title = 'Plot of data with Piece-Wise Linear Regression Estimates using MCMC')
```

