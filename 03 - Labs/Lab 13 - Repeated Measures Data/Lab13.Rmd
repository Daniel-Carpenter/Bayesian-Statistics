---
title: 'Lab 13: Repeated Measures Data'
author: "Daniel Carpenter"
date: "April 2022"
output:
  # html_document: 
  #   df_print: default
  #   theme: cosmo
  #   toc: yes
    # toc_float: yes
  github_document:
    toc: yes
    # number_sections: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Flexible modeling

One of the big advantages of the Bayesian approach is the huge flexibiity around the construction of complex models.

# Blood Pressure
Measurements of blood pressure from twenty healthy individuals was recorded.
In this example there are two repeated measurements.
This example comes from pg 308 ff `Bayesian Modeling Using WinBUGS by Ntzoufras`


```{r data, message = FALSE, warning=FALSE}
dataList=list(n=20, K=2, 
y=structure(.Data=c(108, 98, 91, 94, 93, 96, 104, 99, 99, 97, 95, 98, 93, 97, 99, 96, 90, 100, 92, 95, 101, 89, 97, 97, 97, 100, 96, 95, 106, 100, 100, 98, 90, 99, 88, 98, 92, 92, 100, 101), 
.Dim = c(20, 2) )
)
dataList$y->y
mat=matrix(y,nr=20,nc=2,byrow=TRUE)
mat = cbind(mat, factor(1:20))
colnames(mat)=c("Fst",  "Snd", "Pat") # Pat = patient
df=as.data.frame(mat)
df

df2 = data.frame(Bloodp = c(df$Fst,df$Snd), measurement = rep(c("Fst","Snd"), c(20,20)), patient = factor(rep(df$Pat, 2)))
head(df2)
```




# Task 1

## Plot the data in base R, you can use the data frame `df`:

Use  `boxplot()` and reproduce the plot below.

```{r baserplots, eval = FALSE,echo=FALSE}
boxplot(df[...], col = rainbow(2),main = "Blood pressure - \n two measurements per individual")



```


## Plot the data using ggplot:

Use  `ggplot()` and reproduce the plot below.

```{r ggplots, warning=FALSE, message=FALSE,eval=FALSE, echo=FALSE}
library(ggplot2)
g=ggplot(..., aes(x=measurement,y=Bloodp, fill = measurement) ) + geom_boxplot() + geom_point(aes(col=patient)) + ggtitle("Blood preasure measurements- paired")
g

```



# Classical analysis
This is not a course on classical stats but having some knowledge of the paradigm is definitely helpful. Again this is included  for  completeness.

## Paired samples

The two samples violate the standard assumptions of 2 sample t tests because the samples are *de*pendent. This is  addressed by taking the difference of the samples and testing them with a one sample t test. 
The two tests below are equivalent:

```{r paired}
t.test(Bloodp ~ measurement, paired = TRUE, data = df2)
t.test(df$Fst-df$Snd)
```

The conclusion in both tests is that we should  not reject the NULL hypothesis of 

$$H_0:\mu_{Fst}-\mu_{Snd}=0$$ since the pValue is > 0.05. Note also that the confidence interval contains 0.


## repeated aov
We should obtain the same results as above. 

```{r repeated, eval=TRUE}
av = aov(Bloodp ~ measurement + Error(patient/measurement), data = df2)
summary(av)
```


Please note that the pvalues are the same and in this case $t^2=F$, that is: $(-0.30287)^2=$ `r  (-0.30287)^2`


# Theory



We will need to take into account the variablity within the subject and between subject.

To understand this terminology please read (http://www.statsmakemecry.com/smmctheblog/within-subject-and-between-subject-effects-wanting-ice-cream.html)

There will be correlation between measurements for a particular individual since blood pressure is measured twice on the same patient.


We want to allow the two random variables $Y_{i,1}$ and $Y_{i,2}$ to have covariance $\sigma_a^2$ 

Each datum is the realization of two sources of variablity: 1)  between subject $a_i$ (dependent on i) and within subject $\epsilon_{ij}$.

This means that we can write each response as

$$Y_{ij} = \mu+a_i +\epsilon_{ij}$$

For a given `i` there can only be two values of `j` , hence the two samples are paired.



The $a_i$ term is called a `random effect` and adds or subtracts to the overal mean $\mu$ giving the  mean for the $i^{th}$ individual by taking in consideration the between persons variability.

The following are  within and between variability expressions

$$\epsilon_{ij}\sim N(0,\sigma^2)$$
$$a_i \sim N(0,\sigma_a^2)$$
We could also write the model as 

$$Y_{ij}\sim N(\mu_{ij},\sigma^2)$$ 
where 
$$\mu_{ij}=\mu+a_i$$ 

and 

$$a_i\sim N(0,\sigma_a^2)$$

Notice that $i=1,\ldots,n$ and $j=1,2$.

## Priors 
We can use uniform priors on sigma and then make logical precision nodes from these.

Or we can use  some low impact gamma densities directy on the `precision` nodes (taus) and then make logical nodes to form sigmas. (You will need to do this as one of the tasks below)

# Equivalent model

This is not a requirement for the course because it pre-supposes to much multivariate distributional knowledge. However it is included for completeness.

The following bivariate normal is equivalent to the above model.

$${Y}_i|\mu,\sigma^2  \sim N_2 (\mu 1_2, \Sigma)$$
where
\begin{eqnarray}
\Sigma &=& \left(
 \begin{array}{cc}
  
 \sigma^2+\sigma_a^2 & \sigma_a^2 \\
 \sigma_a^2 & \sigma^2+\sigma_a^2\\
 
 \end{array}\right)
 \end{eqnarray}
 
 
# Expression for correlation 

## Notice the following results:

$$E(Y_{ij})=\mu$$
$$Var(Y_{ij})=Var(\mu +a_{i}+\epsilon_{ij})=Var(a_i)+Var(\epsilon_{ij})=\sigma_a^2 +\sigma^2 $$

$$ Cov(Y_{i1},Y_{i,2})=Cov(\mu+a_i+\epsilon_{i,1},\mu+a_i+\epsilon_{i,2})$$
Because of independence we get:

$$Cov(Y_{i1},Y_{i,2}) = \sigma_a^2$$
 
## Now look at the within correlation

$$r_{12}= Cor(Y_{i1},Y_{i,2}) = \frac{Cov(Y_{i1},Y_{i,2})}{\sigma_a^2+\sigma^2}=\frac{\sigma_a^2}{\sigma_a^2+\sigma^2}$$
 
# The following is a Jags code to perform the simulation

```{r jagscode, warning=FALSE,message=FALSE,eval=FALSE}

library(rjags)

#Define the model:
modelString = "
model {
	for  (i  in 1:n) { 
	       for (j in 1:K){ 
	                 y[i,j] ~ dnorm( mu[i,j], tau )
	                 mu[i,j] <- m + a[i]
		  }
		a[i]~dnorm( 0, tau.a)
    }		
	# prior distributions
	m ~ dnorm( 0.0, 1.0E-06)
	
  sigma ~ dunif(0,200)
	sigma.a ~ dunif(0,200)
  
  tau<-pow(sigma,-2)
  tau.a<-pow(sigma.a,-2)
 
  s2<-1/tau
  s2.a<-1/tau.a

  # correlation between two measurements
r12 <- s2.a/(s2.a+s2)
	# calculation of residuals 
	for  (i  in 1:n) { 
	       for (j in 1:K){ 
			res[i,j]<- y[i,j]-mu[i,j]
	}}
	# Calculation of R2
	R2 <- 1 - pow(  sd(res[1:n,1:K])/sd(y[1:n,1:K]), 2 )
}

"
writeLines( modelString , con="TEMPmodel.txt" )

# close quote for modelStri
#  initsList = list( theta=thetaInit )
initsList = list(m=0.0, sigma=5.0, sigma.a=1.0)

# Run the chains:
jagsModel = jags.model( file="TEMPmodel.txt" , data=dataList , inits=initsList , 
                        n.chains=3 , n.adapt=500 )
#list.samplers(jagsModel)

update( jagsModel , n.iter=1000 )
codaSamples = coda.samples( jagsModel , variable.names=c("r12","m", "s2","s2.a"),
                            n.iter=33330 )
save( codaSamples , file=paste0("lab13","Mcmc.Rdata") )



library(ggmcmc)
s = ggs(codaSamples)
d=ggs_density(s)

print(d)

cr =  ggs_crosscorrelation(s)
print(cr)

summary(codaSamples)

```


## Task 2



### Make  a function that will run this model

### Call it `myrepmeasure()`

### Use different priors.

Put low impact priors on the two taus and  then make sigma nodes that are logically reated to the stochastic  tau nodes


Example: 

```{r, eval=FALSE}
tau ~ dgamma(0.001,0.001)
s2<-pow(tau,-1)

```

You will need to change the code as needed.

### Add some MCMC diagnostics (you could use some of JK's code)


```{r echo=TRUE}
myrepmeasure <- function() {
  
  require(rjags)
  
  #Define the model:
  modelString = "
  model {
  	for  (i  in 1:n) { 
  	       for (j in 1:K){ 
  	                 y[i,j] ~ dnorm( mu[i,j], tau )
  	                 mu[i,j] <- m + a[i]
  		  }
  		a[i]~dnorm( 0, tau.a)
      }		
  	# prior distributions
  	m ~ dnorm( 0.0, 1.0E-06)
  	
    sigma ~ dunif(0,200)
  	sigma.a ~ dunif(0,200)
    
  	tau ~ dgamma(0.001,0.001)
  	s2<-pow(tau,-1)
  	
    # tau<-pow(sigma,-2)
    tau.a<-pow(sigma.a,-2)
   
    # s2<-1/tau
    s2.a<-1/tau.a
  
    # correlation between two measurements
  r12 <- s2.a/(s2.a+s2)
  	# calculation of residuals 
  	for  (i  in 1:n) { 
  	       for (j in 1:K){ 
  			res[i,j]<- y[i,j]-mu[i,j]
  	}}
  	# Calculation of R2
  	R2 <- 1 - pow(  sd(res[1:n,1:K])/sd(y[1:n,1:K]), 2 )
  	
  }
  
  "
  writeLines( modelString , con="TEMPmodel.txt" )
  
  # close quote for modelStri
  #  initsList = list( theta=thetaInit )
  initsList = list(m=0.0, sigma=5.0, sigma.a=1.0)
  
  # Run the chains:
  jagsModel = jags.model( file="TEMPmodel.txt" , data=dataList , inits=initsList , 
                          n.chains=3 , n.adapt=500 )
  #list.samplers(jagsModel)
  
  update( jagsModel , n.iter=1000 )
  codaSamples = coda.samples( jagsModel , variable.names=c("r12","m", "s2","s2.a"),
                              n.iter=33330 )
  save( codaSamples , file=paste0("lab13","Mcmc.Rdata") )
  
  
  # add diagnostics
  source("DBDA2E-utilities.R")
  dir.create('Output//')
  diagMCMC( codaObject=codaSamples , parName="r12" )
  saveGraph( file=paste0('Output//',"r12") , type="png" )

  diagMCMC( codaObject=codaSamples , parName="m" )
  saveGraph( file=paste0('Output//',"m") , type="png" )
  
  diagMCMC( codaObject=codaSamples , parName="s2" )
  saveGraph( file=paste0('Output//',"s2") , type="png" )
  
  diagMCMC( codaObject=codaSamples , parName="s2.a" )
  saveGraph( file=paste0('Output//',"s2.a") , type="png" )
  
  
  library(ggmcmc)
  s = ggs(codaSamples)
  d=ggs_density(s)
  
  print(d)
  
  cr =  ggs_crosscorrelation(s)
  print(cr)
  
  summary(codaSamples)
}
```



### Now run the model 
```{r, cache=TRUE}
myrepmeasure()

# Show the diagnostics
knitr::include_graphics('Output//r12.png',  dpi = 3)
knitr::include_graphics('Output//m.png',    dpi = 3)
knitr::include_graphics('Output//s2.png',   dpi = 3)
knitr::include_graphics('Output//s2.a.png', dpi = 3)
```


#### comment on the quality of the MCMC
* `s2` Overall quality is good. 
  * The param. value's trace has a defined range but is decently spread.
  * Appears to have not too much issue with autocorrelation. 
  * Shrink factor is stable.
  * Chain densities represent each other well
* `m` Overall quality is okay, see not on shrink factor 
  * The param. value's trace has a defined range and is close to mean.
  * No issue with autocorrelation. 
  * Shrink factor has some instability which could be an issue.
  * Chain densities represent each other well
* `s2.a` Overall quality is not reliable
  * The param. value's trace has no defined range and is not close to mean
  * Significant issue with autocorrelation. 
  * Shrink factor stabilizes
  * Chain densities represent some, but not great
* see r12 note below

#### comment on what you can conclude from  `r12`
* `r12` Overall quality is not reliable
  * The param. value's trace has little defined range and is not close to mean
  * Significant issue with autocorrelation. 
  * Shrink factor stabilizes
  * Chain densities represent each some, but not great




